{"0": {
    "doc": "Home",
    "title": "How to share data in the Flanders Smart Data Space",
    "content": "This technical documentation is here to guide you through the pipeline architecture that needs to be set up to share and collect data within the Flanders Smart Data Space. The Flanders Smart Data Space allows you to publish and consume both open and closed data efficiently. For this purpose, a technical standard (Linked Data Event Streams) has been chosen that allows data users to be kept in sync with different dynamic source data sets. It allows data to be exchanged between silos sustainably and cost-effectively using domain-specific ontology for fast and slowly changing data streams. Before exploring how to set up data pipelines, make sure to familiarize yourself with the foundational concepts. This includes understanding what the Flanders Smart Data Space is all about, grasping the Linked Data principle, and knowing what a Linked Data Event Stream (LDES) typically entails. ",
    "url": "/#how-to-share-data-in-the-flanders-smart-data-space",
    
    "relUrl": "/#how-to-share-data-in-the-flanders-smart-data-space"
  },"1": {
    "doc": "Home",
    "title": "Set up your Pipeline, and you are good to go",
    "content": "To either publish your data with or access data from the Flanders Smart Data Space, setting up a pipeline is essential. You can kickstart this process instantly using open-source Java components that are ready to run in a docker container. Are you a data publisher that wants to publish your data in the Flanders Smart Data Space as a Linked Data Event Stream (LDES)? We explain what the publishing pipeline should look like, without going too deeply into the specific configuration of the necessary building blocks. We will assist you in selecting the right open-source building blocks that fit your specific needs, focusing on ease of integration. After this, you can go directly to the tutorials, with the basic concepts and background knowledge in your backpocket. Are you a data consumer eager to delve into your first LDES experience? Setting up a dedicated pipeline is once again essential. This round requires establishing a specialized consumption pipeline designed to sequentially harvest LDES members and propel them downstream. By assembling various components, you can create a customized LDES consuming pipeline tailored to your specific needs. As a data intermediary, your objective is to collect LDES streams and then release a revamped, enriched LDES data stream. The capability to utilize data processing techniques for cleaning, restructuring, or augmenting the initial datasets requires the establishment of a specialized LDES consumption pipeline. Your role involves implementing a resilient and scalable pipeline that not only facilitates an uninterrupted data flow but also aligns with Linked Data principles, thereby transforming your enhanced LDES stream into a crucial resource within the data ecosystem. Publishing Pipeline . Intermediary Pipeline . Consuming Pipeline . ",
    "url": "/#set-up-your-pipeline-and-you-are-good-to-go",
    
    "relUrl": "/#set-up-your-pipeline-and-you-are-good-to-go"
  },"2": {
    "doc": "Home",
    "title": "Learn by doing",
    "content": "Based on a few use cases, we try to teach you in a light-hearted way how to publish or consume an LDES. In these tech docs we do not go into depth about the different possible parameters, but we show a working configuration for the proposed use case. Detailed information about the technical parameters or configuration options can be found under sections of the building blocks individually. Tutorials (Learn by doing) . ",
    "url": "/#learn-by-doing",
    
    "relUrl": "/#learn-by-doing"
  },"3": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"4": {
    "doc": "--> OSLO mapping",
    "title": "--> OSLO mapping",
    "content": " ",
    "url": "/basic/--%20OSLO%20mapping",
    
    "relUrl": "/basic/--%20OSLO%20mapping"
  },"5": {
    "doc": "--> Blog posts",
    "title": "--> Blog posts",
    "content": " ",
    "url": "/basic/--%20blog%20posts",
    
    "relUrl": "/basic/--%20blog%20posts"
  },"6": {
    "doc": "1. Basic concept Linked Data",
    "title": "Basic concepts of linked data",
    "content": ". In 2009, the pioneer of the worldwide web, Tim Berners-Lee, presented a TED talk about linked data as the next phase of the internet, sometimes referred to as the Semantic Web. It is a vision of the evolution of the web of documents into the semantic web, where meaning is added through linked data so that, in addition to the current -human- users of the web, machines and computers can understand and interpret the meaning of data. The Semantic Web allows more advanced and sophisticated search capabilities and machines to understand and process data like humans do. Linked open data is a community project overseen by the W3C organisation and aims to enrich the Web by making open datasets even more accessible through the linked data method. “Linked Open Data is Linked Data which is released under an open license, which does not impede its reuse for free.” Tim Berners-Lee . Tim Berners-Lee has proposed a 5-star system for rating the quality of open data on the Web, with Linked Open Data receiving the highest ranking: . ⭐: data is openly available in some format (e.g. pdf); ⭐⭐: data is available in a structured format(e.g.xls); ⭐⭐⭐: data is available in a non-proprietary structured format (.csv); ⭐⭐⭐⭐: data follows W3C standards, like using RDF and URIs; ⭐⭐⭐⭐⭐: all of the other, plus links to other Linked Open Data sources to provide a context. Linked data is structured data linked to other data by relationships or connections to make it more valuable through semantic searches. It expands on established Web technologies like HTTP and URIs. Still, it uses them to communicate information in a way that computers can read automatically rather than only serving web pages for human readers. The idea behind linked data is to turn the Internet into a global decentralized machine-readable database. The figure above visualizes Resource Description Framework (RDF). RDF is a standard data model for representing and sharing information on the Web, based on the idea of using triples to represent data. A triple consists of three parts: a subject, a predicate, and an object. In the context of linked data, subjects and objects refer to individual pieces of data, such as people, places, or things. Predicates refer to the connections or associations between different nodes, such as the fact that a person is the painter of a painting. In the figure above, the subject http://example.com/person/alice represents Alice, the predicate http://example.com/vocab/isAFriendOf represents the relationship “is a friend of,” and the object http://example.com/person/bob represents Bob. ",
    "url": "/basic/1_linked_data#basic-concepts-of-linked-data",
    
    "relUrl": "/basic/1_linked_data#basic-concepts-of-linked-data"
  },"7": {
    "doc": "1. Basic concept Linked Data",
    "title": "Different types of serialization",
    "content": "Linked Data serialization formats are designed to encode structured data for interchange on the web, enabling the sharing and connecting of data across different sources. Besides Turtle, XML RDF, and N-Quads, several other serialization formats are commonly used in the context of Linked Data and RDF. In the frame of the Flanders Smart Data Spacen, for the moment, only the most commonly used serializations can be chosen in the configuration options of the building blocks. Each format offers unique features and is suitable for different use cases: . JSON-ld . Configuration option building block: application/ld+json . JSON-LD is a lightweight Linked Data format. It is easy for humans to read and write. It is based on the already successful JSON format and provides a way to help JSON data interoperate at Web-scale. JSON-LD is an ideal data format for programming environments, REST Web services, and unstructured databases such as Apache CouchDB and MongoDB. { \"@context\": \"https://json-ld.org/contexts/person.jsonld\", \"@id\": \"http://dbpedia.org/resource/John_Lennon\", \"name\": \"John Lennon\", \"born\": \"1940-10-09\", \"spouse\": \"http://dbpedia.org/resource/Cynthia_Lennon\" } . TURTLE (ttl) . Configuration option building block: text/turtle . Turtle (Terse RDF Triple Language) is a serialization format for expressing data in the Resource Description Framework (RDF) that is designed to be compact and readable by humans. RDF is a standard model for data interchange on the web, which allows data to be linked to other data, enabling a wide range of web-based services. Turtle represents this linked data in a way that emphasizes simplicity and ease of understanding, using a syntax that closely resembles the way triples (subject, predicate, object) are naturally described. In Turtle, each statement (or triple) consists of a subject, a predicate, and an object, followed by a period. It uses prefixes to shorten URIs (Uniform Resource Identifiers), making the data more compact and easier to read. Example: . @base &lt;http://example.org/&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; . @prefix rel: &lt;http://www.perceive.net/schemas/relationship/&gt; . &lt;#green-goblin&gt; rel:enemyOf &lt;#spiderman&gt; ; a foaf:Person ; # in the context of the Marvel universe foaf:name \"Green Goblin\" . &lt;#spiderman&gt; rel:enemyOf &lt;#green-goblin&gt; ; a foaf:Person ; foaf:name \"Spiderman\" . N-Quads . Configuration option building block: application/n-quads . N-Quads is a serialization format designed for RDF (Resource Description Framework) datasets that extends the simplicity and effectiveness of the Turtle format to support named graphs within RDF data. Named graphs are a way of grouping sets of triples, allowing for the representation of multiple, discrete graphs within a single document. This feature is particularly useful for working with complex datasets that involve provenance, versioning, or different viewpoints within the same dataset, facilitating more granular control and organization of RDF data. An N-Quads document consists of a series of statements, where each statement represents a triple (subject, predicate, object) similar to Turtle, with the addition of a fourth element that specifies the graph name (context) to which the triple belongs. Each statement in N-Quads is a line in the file, ending with a period, making it straightforward to parse and generate. &lt;http://one.example/subject1&gt; &lt;http://one.example/predicate1&gt; &lt;http://one.example/object1&gt; &lt;http://example.org/graph3&gt; . # comments here # or on a line by themselves _:subject1 &lt;http://an.example/predicate1&gt; \"object1\" &lt;http://example.org/graph1&gt; . _:subject2 &lt;http://an.example/predicate2&gt; \"object2\" &lt;http://example.org/graph5&gt; . ",
    "url": "/basic/1_linked_data#different-types-of-serialization",
    
    "relUrl": "/basic/1_linked_data#different-types-of-serialization"
  },"8": {
    "doc": "1. Basic concept Linked Data",
    "title": "1. Basic concept Linked Data",
    "content": " ",
    "url": "/basic/1_linked_data",
    
    "relUrl": "/basic/1_linked_data"
  },"9": {
    "doc": "2. Why LDES?",
    "title": "Introduction to Linked Data Event Stream",
    "content": ". Today, the main task of data publishers is to meet the user’s needs and expectations, and they realise this by creating and maintaining multiple querying APIs for their datasets. However, this approach causes various drawbacks for the data publisher. First, keeping multiple APIs online can be costly as the load generated by data consumers is often at the expense of the data publisher. And second, data publishers must ensure their APIs are always up-to-date with the latest standards and technologies, which results in a huge mandatory maintenance effort. As new trends emerge, old APIs may become obsolete and incompatible, creating legacy issues for data consumers who may have to switch to new APIs or deal with outdated data formats and functionalities. Moreover, the existing APIs limit data reuse and innovation since the capabilities and limitations of these APIs constrain data consumers. They can only create their views or indexes on top of the data using a technology they prefer. “Linked Data Event Streams as the base API to publish datasets” . On the other hand, data consumers often have to deal with multiple versions or copies of a dataset that need to be more consistent or synchronised. These versions or copies may have different snapshots or deltas published at other times or frequencies. Although using data dumps provides a data consumer with complex flexibility regarding the needed functionality, this approach also has various drawbacks. For example, data consumers must track when and how each version or copy was created and updated. They also have to compare different versions or copies to identify changes or conflicts in the data. Data consumers may need to be more consistent due to outdated or incomplete versions or copies. They may also miss significant changes or updates in data not reflected in their versions or copies. To overcome these challenges, Linked Data Event Streams (LDES) provide a generic and flexible base API for datasets. With LDES, data consumers can set up workflow to automatically replicate the history of a dataset and stay in sync with the latest updates. ",
    "url": "/basic/2_introduction#introduction-to-linked-data-event-stream",
    
    "relUrl": "/basic/2_introduction#introduction-to-linked-data-event-stream"
  },"10": {
    "doc": "2. Why LDES?",
    "title": "2. Why LDES?",
    "content": " ",
    "url": "/basic/2_introduction",
    
    "relUrl": "/basic/2_introduction"
  },"11": {
    "doc": "3. What is a LDES?",
    "title": "What is a LDES?",
    "content": "The Linked Data Event Stream (LDES) specification (ldes:EventStream) allows data publisher to publish their dataset as an append-only collection of immutable members in its most basic form. Consumers can host one or more in-sync views on top of the default (append-only) view. An LDES is defined as a collection of immutable objects, often referred to as LDES members. These members are described using a specific format called RDF, which stands for Resource Description Framework. RDF is one of the corner stones of Linked Data and on which LDES continues to build. More information on Linked Data can be found here. The LDES specification is based on a hypermedia specification, called the TREE specification. The TREE specification originates from the idea to provide an alternative to one-dimensional HTTP pagination. It allows to fragment a collection of items and interlink these fragments. Instead of linking to the next or previous page, the relation describes what elements can be found by following the link to another fragment. The LDES specification extends the TREE specification by stating that every item in the collection must be immutable. The TREE specification is compatible with other specifications such as activitystreams-core, VOCAB-DCAT-2, LDP, or Shape Trees. For specific compatibility rules, please refer to the TREE specification. LDESes apply — as the term implies — the Linked Data principles to data event streams. A data stream is typically a constant flow of distinct data points, each containing information about an event or change of state that originates from a system that continuously creates data. Some examples of data streams include sensor and other IoT data, financial data, etc. Today, custom code has to be created to integrate data, which makes it rather expensive to integrate multiple data sources. With LDES, a technical standard was created that allows data to be exchanged across silos using domain-specific ontologies. An LDES allows third parties to build specific services (WFS, SPARQL endpoint) themselves on top of their own database that is always in sync with the original dataset. An LDES is a constant flow of immutable objects (such as version objects of addresses, sensor observations or archived representations) containing information changes that originates from a system that continuously creates data. Compared to other event stream specification, the LDES specs opts to publish the entire object for every change. Furthermore, LDES increases the usability and findability of data, as it comes in a uniform Linked Data standard published on an online URI endpoint. As a result, an LDES is self-descriptive meaning and more data can be found by following the links. In a nutshell, there are several reasons why there was a need to develop the Linked Data Event Streams specification: . | Linked Data is a powerful paradigm for representing and sharing data on the Web. Still, it has traditionally focused on representing static data rather than events or changes to that data. | The use of event streams is becoming increasingly prevalent on the Web, as it enables applications to exchange information about changes to data in real-time efficiently. | There was a need for a semantic standard that provides a uniform way to exchange data so that different systems could easily exchange data. | Linked Data Event Streams allow applications to subscribe to a stream of data and receive updates in real-time. | . ",
    "url": "/basic/3_Specification#what-is-a-ldes",
    
    "relUrl": "/basic/3_Specification#what-is-a-ldes"
  },"12": {
    "doc": "3. What is a LDES?",
    "title": "3. What is a LDES?",
    "content": " ",
    "url": "/basic/3_Specification",
    
    "relUrl": "/basic/3_Specification"
  },"13": {
    "doc": "4. Basic structure LDES",
    "title": "Structure of a Linked Data Event Stream",
    "content": "As defined above, an LDES is a collection of members or immutable objects. The LDES spec works both for fast moving data and slow moving data. An example of fast moving data, such as sensor observations, is shown in the example below. The base URI for LDES is https://w3id.org/ldes#, with the preferred prefix being ldes. @prefix ldes: &lt;http://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix sosa: &lt;http://www.w3.org/ns/sosa/&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . &lt;C1&gt; a ldes:EventStream ; ldes:timestampPath sosa:resultTime ; tree:shape &lt;C1/shape.shacl&gt; ; tree:member &lt;observation1&gt; . &lt;observation1&gt; a sosa:Observation ; sosa:resultTime \"2021-01-01T00:00:00Z\"^^xsd:dateTime ; sosa:hasSimpleResult \"...\" . The observation entity (&lt;observation1&gt;) is considered to be immutable, and its existing identifiers can be utilized as such. The specification indicates that an ldes:EventStream should have the following properties: . | tree:member → indicating the members of the collection | tree:shape → a machine-readable description of the members in the collection. Can be SHACL or ShEx. | . Otherwise, an ldes:EventStream may have these properties: . | ldes:timestampPath → indicates how a member precedes another member in the LDES, using a timestamp. | ldes:versionOfPath → indicating the non-version object. See example in the specification. | . As stated above, an LDES can also publish a slow moving dataset, such as street names. An example is shown below. @prefix ldes: &lt;http://w3id.org/ldes#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix dcterms: &lt;http://purl.org/dc/elements/1.1/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix sosa: &lt;http://www.w3.org/ns/sosa/&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . &lt;C1&gt; a ldes:EventStream ; tree:shape &lt;C1/shape.shacl&gt; ; tree:member &lt;streetname1-v1&gt;, &lt;streetname1-v2&gt; . &lt;streetname1-v1&gt; rdfs:label \"Station Road\" ; dcterms:isVersionOf &lt;streetname1&gt; ; dcterms:created \"2020-01-01T00:10:00Z\"^^xsd:dateTime . &lt;streetname1-v2&gt; rdfs:label \"Station Square\" ; dcterms:isVersionOf &lt;streetname1&gt; ; dcterms:created \"2021-01-01T00:10:00Z\"^^xsd:dateTime . This example introduces the concept of versions, because certain entities, such as street names, do not understand the concept of time. In this example, versions of street names are published, ensuring the immutability of the LDES members. When publishing versions of entities, extra information (dcterms:isVersionOf) must be added in order to be able to link these version to an entity. Not introducing versions for entities that do not understand the concept of time would lead to an incorrect implementation of the LDES spec, as shown below. @prefix ldes: &lt;http://w3id.org/ldes#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix dcterms: &lt;http://purl.org/dc/elements/1.1/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix sosa: &lt;http://www.w3.org/ns/sosa/&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . &lt;C1&gt; a ldes:EventStream ; tree:shape &lt;C1/shape.shacl&gt; ; tree:member &lt;streetname1&gt; . &lt;streetname1&gt; rdfs:label \"Station Road\" ; dcterms:created \"2020-01-01T00:10:00Z\"^^xsd:dateTime . &lt;streetname1&gt; rdfs:label \"Station Square\" ; dcterms:created \"2021-01-01T00:10:00Z\"^^xsd:dateTime . In this example, the entity with HTTP URI &lt;streetname1&gt; is not longer immutable, which is a direct conflict with the definition of the LDES spec. It is important to note that once a client processes a member of an LDES, it should never have to process it again. Therefore, a Linked Data Event Stream client can maintain a list of already processed member IRIs in a cache. A reference implementation of a client is available as an open-source SDK as part of the Flanders Smart Data Space initiative. ",
    "url": "/basic/4_basic_structure_LDES#structure-of-a-linked-data-event-stream",
    
    "relUrl": "/basic/4_basic_structure_LDES#structure-of-a-linked-data-event-stream"
  },"14": {
    "doc": "4. Basic structure LDES",
    "title": "4. Basic structure LDES",
    "content": " ",
    "url": "/basic/4_basic_structure_LDES",
    
    "relUrl": "/basic/4_basic_structure_LDES"
  },"15": {
    "doc": "5. Replication & synchronisation",
    "title": "Replication &amp; synchronisation",
    "content": " ",
    "url": "/basic/5_replication#replication--synchronisation",
    
    "relUrl": "/basic/5_replication#replication--synchronisation"
  },"16": {
    "doc": "5. Replication & synchronisation",
    "title": "Replication",
    "content": "To initiate the replication of an LDES, data consumers must configure the LDES client with an LDES endpoint. If multiple views are available for the LDES, the LDES client will begin the replication process utilising the first view it receives. However, suppose a data consumer has a specific preference for a particular view to initiate the replication. In that case, it is also possible to configure the view URI in the LDES client accordingly. When the client visits a fragment, it parses the content to RDF and discovers LDES members by looking for triples with the tree:member. Moreover, the client searches for triples with a tree:relation predicate, signifying links to other fragments, and adds them to its queue of fragments to be fetched if they still need to be retrieved. The client inspects the response headers for each fragment and looks for a potential ‘Cache-control: immutable’ attribute, indicating that the fragment is immutable and does not need to be polled again. ",
    "url": "/basic/5_replication#replication",
    
    "relUrl": "/basic/5_replication#replication"
  },"17": {
    "doc": "5. Replication & synchronisation",
    "title": "Synchronisation",
    "content": "In addition to replication, the LDES client keeps track of all mutable fragments and periodically polls them to check if new LDES members were added. To further optimise the synchronisation process, the LDES client reads the ‘Cache-control: max-age’ value from the response headers, which specifies the time period for which the LDES fragment remains valid. This allows the LDES client to schedule periodic polling more efficiently. By utilising the response headers provided by the LDES server, the LDES client can effectively manage the synchronisation of the LDES stream, while minimising the amount of processing required. This makes the LDES client an efficient and reliable tool for processing and managing Linked Data Event Streams. ",
    "url": "/basic/5_replication#synchronisation",
    
    "relUrl": "/basic/5_replication#synchronisation"
  },"18": {
    "doc": "5. Replication & synchronisation",
    "title": "Resuming",
    "content": "An essential functionality of the LDES client is resuming, enabling the client to halt and resume from where it last stopped. To achieve this functionality, the client utilises an SQLite database to persist the immutable and mutable fragment IDs, guaranteeing that an immutable fragment is only retrieved once. The member IDs are also saved in the database for mutable fragments, ensuring that an LDES member is processed only once. ",
    "url": "/basic/5_replication#resuming",
    
    "relUrl": "/basic/5_replication#resuming"
  },"19": {
    "doc": "5. Replication & synchronisation",
    "title": "5. Replication & synchronisation",
    "content": " ",
    "url": "/basic/5_replication",
    
    "relUrl": "/basic/5_replication"
  },"20": {
    "doc": "8. Metadata catalogue (DCAT)",
    "title": "Metadata catalogue (DCAT)",
    "content": "DCAT is an RDF vocabulary for data catalogues on the Web, enabling easy interoperability and discoverability of metadata for datasets, data services, and portals. It standardises properties for describing datasets, access information, and data services. By using DCAT, publishers can increase their datasets’ exposure and facilitate data sharing and reuse. The LDES Server building block allows to pass a static RDF file on startup, containing DCAT to describe the LDES(es). The server reads and publishes the content. DCAT is an RDF vocabulary designed to facilitate interoperability between data catalogs published on the Web. This document defines the schema and provides examples for its use. DCAT enables a publisher to describe datasets and data services in a catalog using a standard model and vocabulary that facilitates the consumption and aggregation of metadata from multiple catalogs. This can increase the discoverability of datasets and data services. It also makes it possible to have a decentralized approach to publishing data catalogs and makes federated search for datasets across catalogs in multiple sites possible using the same query mechanism and structure. Aggregated DCAT metadata can serve as a manifest file as part of the digital preservation process. For more info on DCAT, visit the DCAT publication . ",
    "url": "/basic/6_DCAT#metadata-catalogue-dcat",
    
    "relUrl": "/basic/6_DCAT#metadata-catalogue-dcat"
  },"21": {
    "doc": "8. Metadata catalogue (DCAT)",
    "title": "8. Metadata catalogue (DCAT)",
    "content": " ",
    "url": "/basic/6_DCAT",
    
    "relUrl": "/basic/6_DCAT"
  },"22": {
    "doc": "9. SHACL validation",
    "title": "SHACL validation",
    "content": "SHACL (Shapes Constraint Language) is a standard for validating RDF data and ensuring that it conforms to a particular structure or shape. In the context of the Linked Data Event Stream (LDES), SHACL shapes are used to provide a machine-readble description of the expected structure of members in the stream. By incorporating SHACL shapes, LDES provides a powerful tool for ensuring data quality and consistency, making it a reliable and trustworthy source of data for various applications. By defining a SHACL shape for the LDES, data producers can ensure that the members they add to the LDES adhere to the required structure, while data consumers can use the shape to validate and reason about the data they receive. As a consequence of the immutability of the members, this shape may evolve, but it must always be backwards compatible to the earlier version. When the new shape is not backwards compatible, a new LDES must be created. SHACL (Shapes Constraint Language) is a standard for validating RDF data and ensuring that it conforms to a particular structure or shape. In the context of the Linked Data Event Stream (LDES), SHACL shapes are used to provide a machine-readable description of the expected structure of members in the stream. By incorporating SHACL shapes, LDES provides a powerful tool for ensuring data quality and consistency, making it a reliable and trustworthy source of data for various applications. By defining a SHACL shape for the LDES, data producers can ensure that the members they add to the LDES adhere to the required structure, while data consumers can use the shape to validate and reason about the data they receive. Defining a shape can be done through the /admin/api/eventstreams/{event stream}/shape endpoint. Example . @prefix sh: &lt;http://www.w3.org/ns/shacl#&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . [] a sh:NodeShape; sh:targetClass &lt;https://w3id.org/ldes#EventStream&gt; ; sh:closed true; sh:ignoredProperties (rdf:type) ; sh:property [ sh:class sh:NodeShape; sh:description \"The schema all elements of the eventstream must conform to.\"@en; sh:maxCount 1; sh:minCount 1; sh:name \"shape\"@en; sh:path &lt;https://w3id.org/tree#shape&gt; ], [ sh:nodeKind sh:IRI ; sh:description \"The object property of the members that idicates how members relate to each other from the time perspective.\"@en; sh:maxCount 1; sh:name \"timestampPath\"@en; sh:path &lt;https://w3id.org/ldes#timestampPath&gt; ], [ sh:nodeKind sh:IRI ; sh:description \"The object property that indicates the object identifier in a version object.\"@en; sh:maxCount 1; sh:name \"versionOfPath\"@en; sh:path &lt;https://w3id.org/ldes#versionOfPath&gt; ] . ",
    "url": "/basic/7_SHACL#shacl-validation",
    
    "relUrl": "/basic/7_SHACL#shacl-validation"
  },"23": {
    "doc": "9. SHACL validation",
    "title": "9. SHACL validation",
    "content": " ",
    "url": "/basic/7_SHACL",
    
    "relUrl": "/basic/7_SHACL"
  },"24": {
    "doc": "Geospatial Fragmentation",
    "title": "Geospatial fragmentation",
    "content": "Geospatial fragmentation will create fragments based on geospatial tiles selected of the fragmentationPath. This allows you to fragment the data on geolocations. ",
    "url": "/basic/fragmentations/geospatial#geospatial-fragmentation",
    
    "relUrl": "/basic/fragmentations/geospatial#geospatial-fragmentation"
  },"25": {
    "doc": "Geospatial Fragmentation",
    "title": "Properties",
    "content": "@prefix tree: &lt;https://w3id.org/tree#&gt; . tree:fragmentationStrategy [ a tree:GeospatialFragmentation ; tree:maxZoom { Mandatory: Required zoom level } ; tree:fragmentationPath { Mandatory: defines which property will be used for bucketizing } ; tree:fragmenterSubjectFilter { Optional: regex to filter the subjects matching the fragmentationPath } ; ] . ",
    "url": "/basic/fragmentations/geospatial#properties",
    
    "relUrl": "/basic/fragmentations/geospatial#properties"
  },"26": {
    "doc": "Geospatial Fragmentation",
    "title": "Algorithm",
    "content": ". | The fragmentationObjects of the member are determined . | We filter the RDF statements where the predicate matches the fragmentationPath. | If an optional regex is provided through the fragmenterSubjectFilter property, we filter on subjects that match this regex. | We select all the object that pass the above filters. | . | A bucket of tiles is created using the coordinates and provided zoomLevel. This is done using the Slippy Map algorithm. | The tiles are iterated. The member is added to every tile, or sub-fragmentations of these tiles. Taking into account: . | A new fragment is created if no fragment exists for the given tile. | There is no memberLimit or max size for a fragment. They do not become immutable. | The member is added to every related fragment. | . | . flowchart TD A[First statement is selected where the predicate matches fragmenterProperty AND subject matches fragmenterSubjectFilter] --&gt; B B[Coordinates of this statement are selected] --&gt; C C[Bucket of tiles is created using the coordinates and zoomLevel] --&gt; D{Next tile?} D --&gt; |true| E{Fragment for tile exists?} E --&gt; |false| F[Create Fragment] E --&gt; |true| G[Add member to fragment] F --&gt; G D --&gt; |false| END[END] G --&gt; D . ",
    "url": "/basic/fragmentations/geospatial#algorithm",
    
    "relUrl": "/basic/fragmentations/geospatial#algorithm"
  },"27": {
    "doc": "Geospatial Fragmentation",
    "title": "Example",
    "content": "Example properties: . @prefix tree: &lt;https://w3id.org/tree#&gt; . tree:fragmentationStrategy [ a tree:GeospatialFragmentation ; tree:maxZoom 15 ; tree:fragmentationPath &lt;http://www.opengis.net/ont/geosparql#asWKT&gt; ; ] . With following example input: . @prefix dc: &lt;http://purl.org/dc/terms/&gt; . @prefix ns0: &lt;http://semweb.mmlab.be/ns/linkedconnections#&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix ns1: &lt;http://vocab.gtfs.org/terms#&gt; . @prefix prov: &lt;http://www.w3.org/ns/prov#&gt; . @prefix ns2: &lt;http://www.opengis.net/ont/geosparql#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix geo: &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt; . &lt;http://njh.me/original-id#2022-09-28T17:11:28.520Z&gt; dc:isVersionOf &lt;http://njh.me/original-id&gt; ; ns0:arrivalStop &lt;http://example.org/stops/402161&gt; ; ns0:arrivalTime \"2022-09-28T07:14:00.000Z\"^^xsd:dateTime ; ns0:departureStop &lt;http://example.org/stops/402303&gt; ; ns0:departureTime \"2022-09-28T07:09:00.000Z\"^^xsd:dateTime ; ns1:dropOffType ns1:Regular ; ns1:pickupType ns1:Regular ; ns1:route &lt;http://example.org/routes/Hasselt_-_Genk&gt; ; ns1:trip &lt;http://example.org/trips/Hasselt_-_Genk/Genk_-_Hasselt/20220928T0909&gt; ; a ns0:Connection ; prov:generatedAtTime \"2022-09-28T17:11:28.520Z\"^^xsd:dateTime . &lt;http://example.org/stops/402161&gt; ns2:asWKT \"POINT (5.47236 50.9642)\"^^ns2:wktLiteral ; a ns1:Stop ; rdfs:label \"Genk Brug\" ; geo:lat 5.096420e+1 ; geo:long 5.472360e+0 . &lt;http://example.org/stops/402303&gt; ns2:asWKT \"POINT (5.49661 50.9667)\"^^ns2:wktLiteral ; a ns1:Stop ; rdfs:label \"Genk Station perron 11\" ; geo:lat 5.096670e+1 ; geo:long 5.496610e+0 . The selected objects would be \"POINT (5.47236 50.9642)\"^^ns2:wktLiteral and \"POINT (5.49661 50.9667)\"^^ns2:wktLiteral . When we convert these coordinates to tiles, the bucket of tiles would be: . | “15/16884/10974” | “15/16882/10975” | . When geospatial fragmentation is the lowest level . After ingestion the member will be part of the following two fragments . | http://localhost:8080/addresses/by-zone?tile=15/16884/10974&amp;pageNumber=1 | http://localhost:8080/addresses/by-zone?tile=15/16882/10975&amp;pageNumber=1 | . ",
    "url": "/basic/fragmentations/geospatial#example",
    
    "relUrl": "/basic/fragmentations/geospatial#example"
  },"28": {
    "doc": "Geospatial Fragmentation",
    "title": "Geospatial Fragmentation",
    "content": " ",
    "url": "/basic/fragmentations/geospatial",
    
    "relUrl": "/basic/fragmentations/geospatial"
  },"29": {
    "doc": "6. Fragmentations and pagination",
    "title": "LDES Fragmentations",
    "content": "To reduce the volume of data that consumers need to replicate or to speed up certain queries, the LDES server can be configured to create several fragmentations. Fragmentations are similar to indexes in databases but then published on the Web. The RDF predicate on which the fragmentation must be applied is defined through configuration. The fragmenting of a Linked Data Event Stream (LDES) is a crucial technique for managing and processing large amounts of data more efficiently. An LDES focuses on allowing clients to replicate a dataset’s history and efficiently synchronise with its latest changes. Linked Data Event Streams may be fragmented when their size becomes too big for one HTTP response. Fragmenting an LDES has two main advantages: . It speeds up certain queries. E.g. an autocompletion client will solve its queries faster using a substring fragmentation than a lineair (append-only) fragmentation It allows data consumers to replicate/stay in sync with only the part of the dataset they are actually interested in. The most basic fragmentation of an LDES is called partitioning, which creates a linear fragmentation, appending the newest members on the latest fragment. ",
    "url": "/basic/fragmentations/index#ldes-fragmentations",
    
    "relUrl": "/basic/fragmentations/index#ldes-fragmentations"
  },"30": {
    "doc": "6. Fragmentations and pagination",
    "title": "Partitioning",
    "content": "By default, every Event Stream will be partitioned, wich means that the LDES server will create fragments based on the order of arrival of the LDES member. The members arriving on the LDES server are added to the first page, while the latest members are always included on the latest page. Algorithm . | The fragment to which the member should be added is determined. | The currently open fragment is retrieved from the database. | If this fragment contains members equal to or exceeding the member limit or no fragment can be found, a new fragment is created instead. | . | If a new fragment is created, the following steps are taken. | The new fragment becomes the new open fragment and the previous fragment becomes immutable1. | This newly created fragment and the previous fragment are then linked with each other by 2 generic relationships1. | The pagenumber of the new fragment is determined based on the old fragment or is set to 1 in case of the first fragment. | . | . 1 In case of the first fragment, a previous fragment does not exist so these steps are skipped. ",
    "url": "/basic/fragmentations/index#partitioning",
    
    "relUrl": "/basic/fragmentations/index#partitioning"
  },"31": {
    "doc": "6. Fragmentations and pagination",
    "title": "Supported Fragmentations:",
    "content": " ",
    "url": "/basic/fragmentations/index#supported-fragmentations",
    
    "relUrl": "/basic/fragmentations/index#supported-fragmentations"
  },"32": {
    "doc": "6. Fragmentations and pagination",
    "title": "6. Fragmentations and pagination",
    "content": " ",
    "url": "/basic/fragmentations/index",
    
    "relUrl": "/basic/fragmentations/index"
  },"33": {
    "doc": "Pagination",
    "title": "Pagination",
    "content": "By default, every Linked Data Event Stream will be partitioned, which means that the LDES server will create fragments based on the order of arrival of the LDES member. The members arriving on the LDES server are added to the first page, while the latest members are always included on the latest page. Algorithm . | The fragment to which the member should be added is determined. | The currently open fragment is retrieved from the database. | If this fragment contains members equal to or exceeding the member limit or no fragment can be found, a new fragment is created instead. | . | If a new fragment is created, the following steps are taken. | The new fragment becomes the new open fragment and the previous fragment becomes immutable. | This newly created fragment and the previous fragment are then linked with each other by 2 generic relationships. | The pagenumber of the new fragment is determined based on the old fragment or is set to 1 in case of the first fragment. | . | . ",
    "url": "/basic/fragmentations/pagination",
    
    "relUrl": "/basic/fragmentations/pagination"
  },"34": {
    "doc": "Timebased Fragmentation",
    "title": "Timebased fragmentation",
    "content": "Timebased fragmentation will create fragments based on a time selected from the fragmentationPath and a given granularity. ",
    "url": "/basic/fragmentations/timebased#timebased-fragmentation",
    
    "relUrl": "/basic/fragmentations/timebased#timebased-fragmentation"
  },"35": {
    "doc": "Timebased Fragmentation",
    "title": "Properties",
    "content": "@prefix tree: &lt;https://w3id.org/tree#&gt; . tree:fragmentationStrategy [ a tree:HierarchicalTimeBasedFragmentation ; tree:maxGranularity { Mandatory: defines the depth level of the fragments } ; tree:fragmentationPath { Mandatory: defines which property will be used for bucketizing } ; tree:fragmenterSubjectFilter { Optional: regex to filter the subjects matching the fragmentationPath } ; ] . For maxGranularity the following values are allowed: . | year, | month, | day, | hour, | minute, | second. | . ",
    "url": "/basic/fragmentations/timebased#properties",
    
    "relUrl": "/basic/fragmentations/timebased#properties"
  },"36": {
    "doc": "Timebased Fragmentation",
    "title": "Algorithm",
    "content": ". | The fragmentationObjects of the member are determined . | We filter the RDF statements where the predicate matches the fragmentationPath. | If an optional regex is provided through the fragmenterSubjectFilter property, we filter on subjects that match this regex. | We select all the objects that pass the above filters. | . | The fragment of the member is determined. For each unit of time starting with year and ending with the chosen granularity from maxGranularity we do the following: . | We take the value of this unit of time from the fragmentationObject. eg: the value of month for 2023-03-02T06:30:40 is 03. | We check if the previous fragment has a child fragment with this value for the unit of time. (In the case of year, the previous fragment is the root fragment.) | If no such fragment exists, a new one is created. | . | The member is added to the last fragment. | . ",
    "url": "/basic/fragmentations/timebased#algorithm",
    
    "relUrl": "/basic/fragmentations/timebased#algorithm"
  },"37": {
    "doc": "Timebased Fragmentation",
    "title": "Example",
    "content": "Example properties: . @prefix tree: &lt;https://w3id.org/tree#&gt; . tree:fragmentationStrategy [ a tree:HierarchicalTimeBasedFragmentation ; tree:maxGranularity \"day\" ; tree:fragmentationPath &lt;http://www.w3.org/ns/prov#generatedAtTime&gt; ; ] . ",
    "url": "/basic/fragmentations/timebased#example",
    
    "relUrl": "/basic/fragmentations/timebased#example"
  },"38": {
    "doc": "Timebased Fragmentation",
    "title": "Timebased Fragmentation",
    "content": " ",
    "url": "/basic/fragmentations/timebased",
    
    "relUrl": "/basic/fragmentations/timebased"
  },"39": {
    "doc": "7. Retention Policies",
    "title": "Retention Policies",
    "content": "To reduce storage fill up, it is possible to set a retention policy per view. A retention policy has to be added together with its view. A retention policy is a set of rules determining how long data should be kept or deleted. A retention policy can be applied to an LDES to manage the storage and availability of data objects over time. Currently, the LDES spec defines two retention policies, a time-based an a version-based retention policy. More information about the retention policies can be found in the spec. The LDES Server buildling block implements a time-based retention policy. ",
    "url": "/basic/retention-policies/index#retention-policies",
    
    "relUrl": "/basic/retention-policies/index#retention-policies"
  },"40": {
    "doc": "7. Retention Policies",
    "title": "Retention polling interval",
    "content": "By default, every day, the server checks if there are members that can be deleted that do not conform to the retention policy anymore. If a higher retention accuracy is desired, or a lower one if resources are limited for example, then a respectively lower or higher retention polling interval can be set via a cron expression. ",
    "url": "/basic/retention-policies/index#retention-polling-interval",
    
    "relUrl": "/basic/retention-policies/index#retention-polling-interval"
  },"41": {
    "doc": "7. Retention Policies",
    "title": "Supported Retention Policies:",
    "content": " ",
    "url": "/basic/retention-policies/index#supported-retention-policies",
    
    "relUrl": "/basic/retention-policies/index#supported-retention-policies"
  },"42": {
    "doc": "7. Retention Policies",
    "title": "7. Retention Policies",
    "content": " ",
    "url": "/basic/retention-policies/index",
    
    "relUrl": "/basic/retention-policies/index"
  },"43": {
    "doc": "Timebased Retention",
    "title": "Timebased Retention Policy",
    "content": "https://w3id.org/ldes#DurationAgoPolicy . Similar to the Point in Time Retention Policy, the Timebased Retention Policy will filter out members based on their ldes:timestampPath. The difference between the previous retention policy is that the Timebased one works with a sliding window, rather than a hard-set value. The sliding window can be defined with a ISO 8601 Temporal Duration. Any members’ ldes:timestampPath that falls outside of this range will be removed. gantt title Timebased Retention (Range: P2D) dateFormat YYYY-MM-DD todayMarker off section Day 1 Current Day: crit, milestone, 2023-11-11, 0d Original Stream: active, 2023-11-08, 3d Sliding Window (Current Day -2 days): 2023-11-9, 2d Stream After Retention Day 1: active, 2023-11-9, 2d section Day 2 Current Day: crit, milestone, 2023-11-12, 0d Original Stream: active, 2023-11-9, 3d Sliding Window (Current Day -2 days): 2023-11-10, 2d Stream After Retention Day 2: active, 2023-11-10, 2d . ",
    "url": "/basic/retention-policies/timebased#timebased-retention-policy",
    
    "relUrl": "/basic/retention-policies/timebased#timebased-retention-policy"
  },"44": {
    "doc": "Timebased Retention",
    "title": "Example",
    "content": "@prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. &lt;view1&gt; a tree:Node ; tree:viewDescription [ a tree:ViewDescription ; ldes:retentionPolicy [ a ldes:DurationAgoPolicy ; tree:value \"PT10M\"^^&lt;http://www.w3.org/2001/XMLSchema#duration&gt; ; ] ; ] . ",
    "url": "/basic/retention-policies/timebased#example",
    
    "relUrl": "/basic/retention-policies/timebased#example"
  },"45": {
    "doc": "Timebased Retention",
    "title": "Timebased Retention",
    "content": " ",
    "url": "/basic/retention-policies/timebased",
    
    "relUrl": "/basic/retention-policies/timebased"
  },"46": {
    "doc": "Version Based Retention",
    "title": "Version Based Retention Policy",
    "content": "https://w3id.org/ldes#LatestVersionSubset . To keep the Event Stream clean with less history, the Version Based Retention Policy allows to only keep a certain amount of versions of a state object (referenced through ldes:versionOfPath). The amount of version to retain can be set as a number (higher than 0). ",
    "url": "/basic/retention-policies/version-based#version-based-retention-policy",
    
    "relUrl": "/basic/retention-policies/version-based#version-based-retention-policy"
  },"47": {
    "doc": "Version Based Retention",
    "title": "Example",
    "content": "@prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. &lt;view1&gt; a tree:Node ; tree:viewDescription [ a tree:ViewDescription ; ldes:retentionPolicy [ a ldes:LatestVersionSubset ; tree:amount 2 ; ] ; ] . ",
    "url": "/basic/retention-policies/version-based#example",
    
    "relUrl": "/basic/retention-policies/version-based#example"
  },"48": {
    "doc": "Version Based Retention",
    "title": "Version Based Retention",
    "content": " ",
    "url": "/basic/retention-policies/version-based",
    
    "relUrl": "/basic/retention-policies/version-based"
  },"49": {
    "doc": "Supported frameworks",
    "title": "Environments",
    "content": ". Currently, we support 2 frameworks to use these building blocks in: . | Apache Nifi: A powerful system to easily process and distribute data | Linked Data Interactions Orchestrator: A lightweight application developed and maintained in the frame of the Flanders Smart Data Space, without the deployment of Apache Nifi. | . ",
    "url": "/pipeline/environements#environments",
    
    "relUrl": "/pipeline/environements#environments"
  },"50": {
    "doc": "Supported frameworks",
    "title": "Option 1: Apache Nifi",
    "content": "Apache NiFi is an open-source data processing and distribution system designed to automate the flow of data between systems. Its user-friendly interface allows for the quick design, control, and monitoring of data flows, making it an invaluable tool for managing complex data pipelines. In the context of Linked Data Event Streams (LDES), Apache NiFi can be used to consume, process, and send linked data members efficiently downstream, across different systems and environments. With its support for various data formats and protocols, NiFi can handle the intricacies of linked data, such as ensuring data integrity, maintaining semantic relationships, and facilitating the real-time processing of data streams. By leveraging NiFi’s capabilities, users can streamline the ingestion, transformation, and dissemination of linked data, thereby enhancing the accessibility and utility of information within the Flanders Smart Data Space or similar environments. This makes Apache NiFi a powerful ally in harnessing the full potential of LDES, enabling organizations to unlock valuable insights from their data ecosystems. ",
    "url": "/pipeline/environements#option-1-apache-nifi",
    
    "relUrl": "/pipeline/environements#option-1-apache-nifi"
  },"51": {
    "doc": "Supported frameworks",
    "title": "Option 2: Linked Data Interactions Orchestrator",
    "content": "Deploying Apache Nifi can sometimes be undesirable due to the high deployment costs involved. In response to this challenge, a lightweight application named LDIO has been developed and maintained within the Flanders Smart Data Space framework. LDIO is designed to offer a cost-effective alternative, significantly reducing the resources and financial investment required for deployment. ",
    "url": "/pipeline/environements#option-2-linked-data-interactions-orchestrator",
    
    "relUrl": "/pipeline/environements#option-2-linked-data-interactions-orchestrator"
  },"52": {
    "doc": "Supported frameworks",
    "title": "Which environment should I use?",
    "content": "Error handling . You would turn to the Apache NiFi environment when aiming to establish a robust data pipeline, especially when maintaining control over improperly handled data members becomes a priority. This scenario might include instances like incorrect geometry in the GeoJson-to-WKT (Well-Known Text) Processor, an LDES member failing to validate against a SHACL (Shapes Constraint Language) shape, among other potential issues. Apache NiFi’s appeal lies in its comprehensive toolkit that allows for intricate control over data flows, including error handling mechanisms that enable users to identify, diagnose, and correct data processing anomalies. By using NiFi, you can ensure that each piece of data, regardless of its complexity or the intricacies involved in its processing, is managed effectively. This level of oversight and control is critical in environments where data accuracy, consistency, and reliability are paramount, making Apache NiFi an ideal choice for handling sophisticated data pipelines that demand rigorous error management and data integrity assurance. If you’re looking to quickly set up a data pipeline without the immediate need to build a safety net for instances where a data member is improperly processed by a component, we recommend using the Linked Data Interactions environment. This environment is designed for efficiency and ease of use, allowing you to streamline the pipeline construction process. It is particularly suitable for scenarios where the primary focus is on rapid development and deployment, and less on the intricate handling of data errors at the outset. The Linked Data Interactions environment offers a straightforward approach, making it an ideal choice for those who prioritize speed and simplicity in their data pipeline setup. Nevertheless, there is a debug logging option in LDIO, so it becomes possible to track and analyze detailed operational information, helping developers and system administrators diagnose issues, understand application behavior, and optimize performance more effectively. Monitoring logging . To provide a better insight in the workings in the LDIO, a Prometheus endpoint is exposed that encloses some metrics. In this way, developers and system administrators can monitor the system’s performance and reliability in real-time, enabling quick identification and resolution of any issues that may arise. ",
    "url": "/pipeline/environements#which-environment-should-i-use",
    
    "relUrl": "/pipeline/environements#which-environment-should-i-use"
  },"53": {
    "doc": "Supported frameworks",
    "title": "Supported frameworks",
    "content": " ",
    "url": "/pipeline/environements",
    
    "relUrl": "/pipeline/environements"
  },"54": {
    "doc": "General concept",
    "title": "General concept",
    "content": "To either publish your data with or access data from the Flanders Smart Data Space, setting up a pipeline is essential. You can kickstart this process instantly using open-source Java components that are ready to run in a docker container. ",
    "url": "/pipeline/general_concept",
    
    "relUrl": "/pipeline/general_concept"
  },"55": {
    "doc": "General concept",
    "title": "Publisher pipeline",
    "content": "Are you a data publisher that wants to publish your data in the Flanders Smart Data Space as a Linked Data Event Stream (LDES)? In that case, it is necessary to configure a publishing pipeline. For instance, when a data publisher aims to distribute non-linked data as an LDES, a specifically configured publisher pipeline manages the entire process. Initially, an adapter component transforms the data into linked data. Subsequently, a transformer component converts the geometry into WKT format. Once the data is prepared for publication, it is transmitted to the LDES server using an HTTP-out component. It’s important to note that the LDES server is not part of the pipeline; rather, it functions as a separate VSDS building block. For effective data publication, two services need to be deployed. ",
    "url": "/pipeline/general_concept#publisher-pipeline",
    
    "relUrl": "/pipeline/general_concept#publisher-pipeline"
  },"56": {
    "doc": "General concept",
    "title": "Consuming pipeline",
    "content": "Are you a data consumer eager to delve into your first LDES experience? Setting up a dedicated consumer pipeline is once again essential. For instance, if a data user intends to access data from an LDES and store it in a database (such as GraphDB), they will need to establish a consuming data pipeline. Initially, the configuration of the LDES client component is required. This component, an integral part of the data pipeline, sequentially retrieves each LDES member and forwards them through the process. Following this, a Repository Materializer component must be integrated into the pipeline to enable the writing of LDES members to the GraphDB. ",
    "url": "/pipeline/general_concept#consuming-pipeline",
    
    "relUrl": "/pipeline/general_concept#consuming-pipeline"
  },"57": {
    "doc": "General concept",
    "title": "Intermediary pipeline",
    "content": "As a data intermediary, your objective is to collect LDES streams and then release a revamped, enriched LDES data stream. The capability to utilize data processing techniques for cleaning, restructuring, or augmenting the initial datasets requires the establishment of a specialized LDES consumption pipeline. Your role involves implementing a resilient and scalable pipeline that not only facilitates an uninterrupted data flow but also aligns with Linked Data principles, thereby transforming your enhanced LDES stream into a crucial resource within the data ecosystem. ",
    "url": "/pipeline/general_concept#intermediary-pipeline",
    
    "relUrl": "/pipeline/general_concept#intermediary-pipeline"
  },"58": {
    "doc": "Performance",
    "title": "Performance",
    "content": "The LDES Server and LDIO Workbench are components for publishing data as LDES. They are available as docker images that can be deployed locally or in the cloud using any container technology such as docker compose, kubernetes, or similar mechanism provided by cloud providers. How you size the docker containers towards available CPU and memory impacts the performance of these components. Depending on your specific needs you will have to use more or less CPU and/or memory to alow your specific data sets to be ingested slower or faster into the LDES Server’s database. Likewise, you need to balance these sizes for the amount of data you have and the number of clients that will be requesting this data. It is very difficult to predict the best sizes for CPU and memory usage in any particular case, especially if data sets are added or removed. We defined and executed a number of performance tests to measure how fast these components work given some limitations in order to have a base line that can be used for sizing your particular setup. ",
    "url": "/pipeline/performance",
    
    "relUrl": "/pipeline/performance"
  },"59": {
    "doc": "Performance",
    "title": "LDES Server Performance",
    "content": "The LDES Server allows to ingest the members of a data set (LDES), stores them in a database and allows to retrieve the members in a partitioned way (view). The way the LDES server is designed, these three features are decoupled from one another. That is, the ingesting &amp; storing happens immediately upon data reception, while the partitioning happens as a background task. Once the data is processed by this job it is available for retrieval. Depending on your data set you may first ingest all members by only defining a LDES without any views and than defining one or more views to partition the data set and make it available for retrieval. Note that without a view, the data set cannot be retrieved. Alternatively, you can define the view(s) before ingesting the members resulting in the data set being partitioned (in the background) while ingesting. For data sets that do not change (e;g. historical data sets), it depends on how fast after ingest the data set should be available: you may choose to fragment the data set during ingest or only after the data set is ingested. For fast moving data sets we recommend to ingest and partition in parallel to ensure the data set is available as soon as possible. In order to test the server’s performance we defined a test plan, setup a controlled test environment, created a number of tests and executed these tests while collecting some basic statistics. If you only care about the numbers, you can skip the following sections and find the results below. What and why? - Test Plan . Data sets come in all shapes and sizes, some are small while other are huge and some are fast, slow or even non-moving. Obviously, this impacts how fast you need to be able to ingest and store the data. As said, the ingesting and storing runs separately from the partitioning (fragmentation) and we can configure the system in such a way that it does these two tasks either separately or together. So, we measure the ingest speed and the fragmentation speed both sequentially and in parallel. In addition, as the LDES server can accept multiple HTTP connection at the same time we also measure the effect of ingesting data from one vs. multiple clients/pipelines. For the partitioning performance measuring we vary on the number of views to measure the impact of multiple simultaneous fragmentations. As there are multiple types of fragmentation (i.e. simply paged, geospatial &amp; time-based) we measure the difference in speed of these types so you know the impact and can choose how and when to offer a(n additional) view on the data set. When creating a view you need to specify the number of members that each fragment will contain. The optimal amount will vary on the size of your members (number of linked data triples) as this will impact the size of a fragment and therfore the amount of time needed when returning the fragment and the time needed to actually read (parse) the fragment. In order to determine the optimal member count (for a specific use case) we measured the retrieve (fetch) time needed varying on the number of members in a fragment. Where? - Test Environment . As said, the LDES components are available as docker images and can therefore be deployed locally or in a cloud environment. We used the local environment for test development and to simulate a high-end deployment environment. We used a typical cloud environment as a mid-range deployment environment. Finally, we also ran the tests in a very small physical environment. This allowed us to measure the impact of the differently sized environments. For all test enviroments we used the same sizing for the actual containers: . | sizes | initial | limit | . | cpu (virtual) | 0.5 | 3.0 | . | memory (GiB) | 0.5 | 2.0 | . Note: in the cloud environment we had to increase the maximum memory to 3 GB for one test (retrieve 10000 members/page) to avoid running out of java heap space. High-End Test Environment . To simulate a high-end deployment environment we used a MacBook Pro (Apple M2 pro with 10 cores - 6 performance + 4 efficiency) with 16GiB physical RAM. Using docker compose we ran all our services locally on this system in their own private network. Mid-range Test Environment . For a typical deployment environment we used a cloud environment where we had two separate clusters: one for the (postgres) database and one for all our other services. The database cluster is a single node t4g.medium instance(2 vCPU and 4 GiB system), while the other services used a dual node m5.2xlarge instance (8 vCPU and 32 GiB system). We used ArgoCD and terraform for the deployment in this environment. Low-End Test Environment . Some environments may not need the raw power or may even have a very tight budget. For that reason we also tested the performance on a Raspberry PI model 4 with 8GiB RAM (quad-core), deploying all services locally using docker compose. How? - Test Method . To write and run the actual tests we used JMeter. In addition, we created and used a small management tool (JMeter Runner) to schedule tests, cancel or delete tests, follow up the running tests and view tests results. Each test ingests 100K members using a number of pipelines and partitions the data set using one or more views. A test produces a stats.xml file containing the base measurements in addition to the standard report generated automatically by JMeter. This stats.xml file allows us to capture and report measurements on the background (fragmentation) tasks as well. We run the following tests in seven test runs to have a few statistical values, i.e. min, max and average: . | calculate ingest speed using 1, 2 and 4 pipelines - sequentially (without partitioning by only seeding the LDES definition) | calculate fragmentation speed using 1, 2 and 4 simple paged views - sequentially (after sequential ingest) | calculate fragmentation speed of geospatial (by-location) and time-based (by-time) using 1 view | calculate ingest and fragmentation speed using a combination of 1, 2 &amp; 4 pipelines vs. 1, 2 &amp; 4 views (e.g. 2 pipelines towards 4 views) - parallel ingest &amp; partitioning | calculate fetch speed using 10, 100, 250, 500, 1000, 2500, 5000 &amp; 10000 members per page - no parsing/reading, measures only the fragment creation and retrieval | . The test definitions and the instructions to run these tests locally can be found here while the docker compose file and related files can be found one level up. Results . All results show the average values of all test runs in members per second. For details we would like to refer you to the results source. We have tested our reference image (3.5.0) of the LDES Server. Note that the low-end numbers are not yet available and may change the conclusions below. Sequential Ingest Speed Test . |   | High-end | Mid-range | Low-end | . | 1 pipeline | 2,451 | 883 |   | . | 2 pipelines | 4,348 | 1,525 |   | . | 4 pipelines | 4,928 | 1,919 |   | . Note that we see a performance increase of 70% to 100% for the high-end and even up to 120% increase for the mid-range environment when multiple pipelines are used on a system which has enough cores. The original target ingest rate of 100K members/minute is feasible and even largely surpassed with 147K members/minute. This proves that the LDES Server ingest task is fast enough given enough resources. So, given that enough memory is assigned to the LDES server, you can use multiple pipelines to ingest the same or different data set at the same time. Sequential Fragment Speed Test . |   | High-end | Mid-range | Low-end | . | 1 view | 1,975 | 329 |   | . | 2 views | 1,530 | 309 |   | . | 4 views | 694 | 260 |   | . Note that the performance increase is 55% &amp; 41% (high-end case) and 88% &amp; 216% (mid-range case) for 2 resp. 4 views at once. Although the raw numbers seem to indicate a performance loss, the work done for 2 and 4 views is double resp. quadruple. So, there is in fact an increase in performance when doing the fragmentation for the views in parallel vs. if we would have to fragment multiple views sequentially. This means that it is faster to fragment all the views at once, so if you know that you will need multiple views, you can define them all before ingestion. Parallel Ingest Speed Test . |   |   | High-end | Mid-range | Low-end | . | 1 pipeline | 1 view | 2,394 | 835 |   | . | 2 pipelines | 1 view | 4,204 | 1,451 |   | . | 4 pipelines | 1 view | 4,997 | 1,933 |   | . | 1 pipeline | 2 views | 1,970 | 775 |   | . | 2 pipelines | 2 views | 3,242 | 1,363 |   | . | 4 pipelines | 2 views | 4,286 | 1,891 |   | . | 1 pipeline | 4 views | 1,046 | 693 |   | . | 2 pipelines | 4 views | 1,965 | 1,229 |   | . | 4 pipelines | 4 views | 3,329 | 1,779 |   | . Note that the ingest speed is typically slower if done in parallel with the fragmentation task, with the ingest speed decreasing more when fragmenting multiple views. The performance decrease is up to 57% for high-end systems when ingesting using a single pipeline and fragmenting 4 views. For a mid-range system this is only 22%. But as the ingest speed is overall larger than the fragmentation speed, it may still be a good choice to ingest in parallel. Parallel Fragmentation Speed Test . |   |   | High-end | Mid-range | Low-end | . | 1 view | 1 pipeline | 1,728 | 316 |   | . | 2 views | 1 pipeline | 1,411 | 288 |   | . | 4 views | 1 pipeline | 631 | 240 |   | . | 1 view | 2 pipelines | 2,007 | 314 |   | . | 2 views | 2 pipelines | 1,394 | 289 |   | . | 4 views | 2 pipelines | 624 | 242 |   | . | 1 view | 4 pipelines | 1,807 | 312 |   | . | 2 views | 4 pipelines | 1,380 | 286 |   | . | 4 views | 4 pipelines | 581 | 241 |   | . Note that the fragmentation speed is also slower than the ingest speed when doen in parallel but it is less noticable with a decrease of up to 16% for a high-end system and only up to 8% for a mid-range one. Because the partitioning is not that much slower when done in parallel we recommend to do the ingest and fragmentation in parallel unless really needed. You can do them sequentially as described above. Fetch Speed Test . Note that we are aware of some improvements which may affect the fetch measurements below in a positive way (possibly by 40% - 60%). |   | High-end | Mid-range | Low-end | . | 10 mpp | 2,242 | 698 |   | . | 100 mpp | 3,051 | 1,342 |   | . | 250 mpp | 3,051 | 1,434 |   | . | 500 mpp | 2,984 | 1,375 |   | . | 1000 mpp | 2,844 | 1,252 |   | . | 2500 mpp | 2,500 | 1,179 |   | . | 5000 mpp | 2,470 | 1,024 |   | . | 10000 mpp | 2,345 | 1,121 |   | . Note that the optimal fetch speed is 250 members per page (for a linked data model of about 30 triples). Using a small page size (e.g. 10 mpp) is 25% - 50% slower. Using larger page size (&gt; 1000 mpp) typically decreases the fetch rate by 20% - 30 %. The ideal page size is 100 mpp - 1000 mpp for a 30 triples data model meaning that the ideal fragment size is order of magnitude of thousands to ten thousands of triples. You may need to experiment a bit by creating a few views with a different page member count, but the usual default that we recommend is 250 mpp. ",
    "url": "/pipeline/performance#ldes-server-performance",
    
    "relUrl": "/pipeline/performance#ldes-server-performance"
  },"60": {
    "doc": "Performance",
    "title": "LDES Client Performance",
    "content": "Not yet available . ",
    "url": "/pipeline/performance#ldes-client-performance",
    
    "relUrl": "/pipeline/performance#ldes-client-performance"
  },"61": {
    "doc": "Release management",
    "title": "Release management",
    "content": " ",
    "url": "/pipeline/release",
    
    "relUrl": "/pipeline/release"
  },"62": {
    "doc": "Release management",
    "title": "LDES Server ",
    "content": "Versions: . | Latest Official version: | Latest Alpha version: | . Go to the LDES Server Github Releases for all the releases. ",
    "url": "/pipeline/release#ldes-server-",
    
    "relUrl": "/pipeline/release#ldes-server-"
  },"63": {
    "doc": "Release management",
    "title": "Linked Data Interactions ",
    "content": "The Linked Data Interactions Repo (LDI) is a bundle of basic SDKs used to receive, generate, transform and output Linked Data. This project is set up in the context of the VSDS Project to ease adopting LDES on data consumer and producer side. Go to the LDI Github Releases for all the releases. Linked Data Interactions Orchestrator (LDIO) . The Linked Data Interactions Orchestrator is built to be a lightweight Linked Data pipeline framework. For more detail on how to use the LDIO, please visit the LDIO Documentation. Versions: . | Latest Official version: | Latest Alpha version: | . Linked Data Interactions for Apache NiFi . To also support existing users of Apache NiFi, all LDI components have been made available as NiFi component. For that, a specially bundled NiFi Docker image is available. Documentation on how to use the individual Processors can be found in the in-app documentation and on the LDI NiFi docs. Versions: . | Latest Official version: | Latest Alpha version: | . ",
    "url": "/pipeline/release#linked-data-interactions-",
    
    "relUrl": "/pipeline/release#linked-data-interactions-"
  },"64": {
    "doc": "Release management",
    "title": "Which version should I use?",
    "content": ". | As a general recommendation, the official release is the goto version to use. This is the ideal version to use on Production. | The alpha/snapshot versions available on dockerhub provide a certain stability, but have not been internally validated on specification compliance. These versions can be used for non-production environments (test/acceptance/…) | The images found on GitHub itself should not be used for anything other than testing, they are prone to changes and do not come with any support. | . ",
    "url": "/pipeline/release#which-version-should-i-use",
    
    "relUrl": "/pipeline/release#which-version-should-i-use"
  },"65": {
    "doc": "Release management",
    "title": "Notification of new releases",
    "content": "To get informed when a new version of a building block is released, go to ‘watch’. On the notifications page, you can see your personal notification subscribtions . ",
    "url": "/pipeline/release#notification-of-new-releases",
    
    "relUrl": "/pipeline/release#notification-of-new-releases"
  },"66": {
    "doc": "5. LDES server",
    "title": "LDES server",
    "content": "LDIO Pipeline see reference guide Apache Nifi Component Name: `` see Apache Nifi reference guide . The Linked Data Event Stream (LDES) server is a configurable component that can be used to ingest, store, and (re-)publish one or multiple Linked Data Event Stream(s). The open-source LDES server is built in the context of the VSDS project to exchange (Open) Data easily. The server can be configured to meet the organisation’s specific needs. Functionalities include retention policy, fragmentation, deletion, create a snapshot and pagination for managing and processing large amounts of data more efficiently and ensuring the efficient use of storage. The LDES server is available as on open-source building block on GitHub . ",
    "url": "/publisher/LDES_server#ldes-server",
    
    "relUrl": "/publisher/LDES_server#ldes-server"
  },"67": {
    "doc": "5. LDES server",
    "title": "Setting up the LDES Server during startup process",
    "content": "Ingesting sources (HTTP in) . The LDES server is able to receive data via HTTP ingestion. Specifically, the server expects a single object (member) to be sent as input via a POST request. If the dataset still contains state objects, each of these must first be converted to a version object before being ingested in the server. Once the objects in the dataset are LDES-compliant members (whether or not after conversion to a version object) and the LDES member has been added to the LDES server, the server can effortlessly publish the LDES member as part of the LDES. More information on the HTTP ingestion can be found here. Example HTTP Ingest-Fetch Configuration: . server.port: { http-port } ldes: collection-name: { short name of the collection, cannot contain characters that are used for url interpretation, e.g.’ $’, ‘=’ or ‘&amp;’, } host-name: { hostname of LDES Server } member-type: { Defines which syntax type is used to define the member id e.g. “https://data.vlaanderen.be/ns/mobiliteit#Mobiliteitshinder”, } timestamp-path: { SHACL property path to the timestamp when the version object entered the event stream., } version-of: { SHACL property path to the non-versioned identifier of the entity. } validation: shape: { URI to defined shape } enabled: { Enables/Disables shacl validation on ingested members } rest: max-age: { time in seconds that a mutable fragment can be considered up-to-date, default when omitted: 60, } max-age-immutable: { time in seconds that an immutable fragment should not be refreshed, default when omitted: 604800, } . SHACL . The LDES specification prescribes that each LDES must link to a SHACL shape, providing a machine-readable definition of the members in the collection. If a SHACL shape was provided on startup, the LDES server reads it before the ingestion process starts and the SHACL shape is used to validate the ingested members. Only valid members are ingested in the LDES server. When starting the server, it is possible to provide a SHACL shape via through an RDF file. At last, the SHACL shape is also published as part of the LDES on the Web. SHACL stands for Shapes Constraint Language and is used to define a set of constraints which are used to verify to conformity of RDF data with these constraints. The SHACL shape specifies the expected properties of an LDES members and the constraints that must be followed to ensure the LDES member adheres to the expected structure and semantics. It defines properties such as required properties, allowed property values, and the data types expected for the properties. More information on how to provide an RDF file, containing a SHACL shape, to the LDES server can be found here. Fragmentation . To reduce the volume of data that consumers need to replicate or to speed up certain queries, the LDES server can be configured to create several fragmentations. Fragmentations are similar to indexes in databases but then published on the Web. The RDF predicate on which the fragmentation must be applied is defined through configuration. The fragmenting of a Linked Data Event Stream (LDES) is a crucial technique for managing and processing large amounts of data more efficiently. There are three main methods of fragmentation: geospatial, time-based, and substring fragmentation. . Partitioning . When applying partitioning, the LDES server will create fragments based on the order of arrival of the LDES member, and is a linear fragmentation. This fragmentation is considered the most basic and default fragmentation because it stands for the exact reason the LDES specification was created: replication and synchronising with a dataset. The members arriving on the LDES server are added to the first page, while the latest members are always included on the latest page. The expected parameter to apply a partioning is a member limit, indicating the amount of members that can be added to each page before creating a new page. name: “pagination” config: memberLimit: { Mandatory: member limit &gt; 0 } . Algorithm . | The fragment to which the member should be added is determined. | The currently open fragment is retrieved from the database. | If this fragment contains members equal to or exceeding the member limit or no fragment can be found, a new fragment is created instead. | . | If a new fragment is created, the following steps are taken. | The new fragment becomes the new open fragment and the previous fragment becomes immutable1. | This newly created fragment and the previous fragment are then linked with each other by 2 generic relationships1. | The pagenumber of the new fragment is determined based on the old fragment or is set to 1 in case of the first fragment. | . | . 1 In case of the first fragment, a previous fragment does not exist so these steps are skipped. Example properties . name: \"pagination\" config: memberLimit: 10 . Substring fragmentation . Substring fragmentation is currently no longer supported as a fragmentation option in the LDES server. Time-based fragmentation . Time-based fragmentation has not yet been implemented. Example of a time-based fragmentation configuration file . name: “timebased” config: memberLimit: { member limit &gt; 0 } . Algorithm This fragmentiser will create an initial fragment with the current timestamp when processing a member. Members are added to the fragment until the member limit is reached. When the fragment member limit is reached, a next fragment is created with a new current timestamp. Reasons for deprecating this fragmentiser: . | This fragmentiser follows the algorithm of pagination but without the semantics. | For a correct timebased fragmentation, members of the fragment should be checked and their value for a given property should be used to create the correct relations. This is not the case, and there is currently no demand to have this implemented. | . Geospatial fragmentation . Consider the scenario where the address registry is published as an LDES that using partitioning. In such a case, data consumers are required to replicate the entire linear set of fragments, despite only being interested in a smaller subset of the dataset. For instance, the city of Brussels may only require addresses within its geographical region and is not interested in other addresses. However, with the partitioned LDES, they would need to iterate through all the fragments and filter the LDES members (address version objects) on the client-side. By utilising geospatial fragmentation, the data can be divided into smaller pieces (tiles) based on geographical location. This facilitates filtering on the fragment level (tiles) and allows for processing and analysis of data within specific geospatial tiles. The geospatial fragmentation supported by the LDES server is based on the “Slippy Maps” algorithm. The fragmentation expects a zoom level parameter which is used by the algorithm to divide the “world” into tiles. The number of tiles if \\(2^{2n}\\) (where n = zoom level). The second expected parameter is an RDF predicate, indicating on which property of the LDES member the fragmentation should be applied. More information about the algorithm used to apply a geospatial fragmentation can be found here. The required configuration for this fragmentation is: . | RDF predicate on which the fragmentation should be based . | Zoom level . | . Example of geospatial fragmentation configuration file . name: “geospatial” config: maxZoomLevel: { Required zoom level } fragmenterProperty: { Defines which property will be used for bucketizing } . Algorithm . | The fragmentationObjects of the member are determined . | We filter the RDF statements where the predicate matches the fragmenterProperty | If an optional regex is provided through the fragmenterSubjectFilter property, we filter on subjects that match this regex. | We select all the object that pass the above filters. | . | A bucket of tiles is created using the coordinates and provided zoomLevel. This is done using the Slippy Map algorithm. | The tiles are iterated. The member is added to every tile, or sub-fragmentations of these tiles1. Taking into account: . | A new fragment is created if no fragment exists for the given tile. | There is no memberLimit or max size for a fragment. They do not become immutable. | The member is added to every related fragment | . | . 1 If the geospatial fragmentation is not the lowest fragmentation level, the member is not added to the tile but to a subfragment on this tile. This case is included in the example below. Example . Example properties: . name: \"geospatial\" config: maxZoomLevel: 15 fragmenterProperty: \"http://www.opengis.net/ont/geosparql#asWKT\" . With following example input: . @prefix dc: &lt;http://purl.org/dc/terms/&gt; . @prefix ns0: &lt;http://semweb.mmlab.be/ns/linkedconnections#&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix ns1: &lt;http://vocab.gtfs.org/terms#&gt; . @prefix prov: &lt;http://www.w3.org/ns/prov#&gt; . @prefix ns2: &lt;http://www.opengis.net/ont/geosparql#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix geo: &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt; . &lt;http://njh.me/original-id#2022-09-28T17:11:28.520Z&gt; dc:isVersionOf &lt;http://njh.me/original-id&gt; ; ns0:arrivalStop &lt;http://example.org/stops/402161&gt; ; ns0:arrivalTime \"2022-09-28T07:14:00.000Z\"^^xsd:dateTime ; ns0:departureStop &lt;http://example.org/stops/402303&gt; ; ns0:departureTime \"2022-09-28T07:09:00.000Z\"^^xsd:dateTime ; ns1:dropOffType ns1:Regular ; ns1:pickupType ns1:Regular ; ns1:route &lt;http://example.org/routes/Hasselt_-_Genk&gt; ; ns1:trip &lt;http://example.org/trips/Hasselt_-_Genk/Genk_-_Hasselt/20220928T0909&gt; ; a ns0:Connection ; prov:generatedAtTime \"2022-09-28T17:11:28.520Z\"^^xsd:dateTime . &lt;http://example.org/stops/402161&gt; ns2:asWKT \"POINT (5.47236 50.9642)\"^^ns2:wktLiteral ; a ns1:Stop ; rdfs:label \"Genk Brug\" ; geo:lat 5.096420e+1 ; geo:long 5.472360e+0 . &lt;http://example.org/stops/402303&gt; ns2:asWKT \"POINT (5.49661 50.9667)\"^^ns2:wktLiteral ; a ns1:Stop ; rdfs:label \"Genk Station perron 11\" ; geo:lat 5.096670e+1 ; geo:long 5.496610e+0 . The selected objects would be . \"POINT (5.47236 50.9642)\"^^ns2:wktLiteral and \"POINT (5.49661 50.9667)\"^^ns2:wktLiteral . When we convert these coordinates to tiles, the bucket of tiles would be: . | “15/16884/10974” | “15/16882/10975” | . When geospatial fragmentation is the lowest level . After ingestion the member will be part of the following two fragments . | http://localhost:8080/addresses/by-zone?tile=15/16884/10974 | http://localhost:8080/addresses/by-zone?tile=15/16882/10975 | . When we have a timebased sub-fragmentation below geospatial fragmentation . After ingestion the member will be part of the following two fragments . | http://localhost:8080/addresses/by-zone-and-time?tile=15/16884/10974&amp;generatedAtTime=2023-02-15T10:14:28.262Z | http://localhost:8080/addresses/by-zone-and-time?tile=15/16882/10975&amp;generatedAtTime=2023-02-15T10:14:28.262Z | . Note that the generatedAtTime=2023-02-15T10:14:28.262Z is an example, this can be any other fragmentation. . Combining geospatial fragmentation and partioning . The LDES server typically adds an LDES member to the “lowest” possible fragment, which in the case of geospatial fragmentation, would be the fragment representing the corresponding geospatial tile. However, some fragments/tiles may have many members, while others may have none. In the worst-case scenario, all members may be added to one geospatial tile/fragment, leading to an enormous fragment. Combining geospatial fragmentation with partitioning can be a useful approach to mitigate this issue. Then partitioning is applied within every geospatial tile, resulting in a set of linear fragments for every geospatial tile. Doing so, all members can still end up in the same geospatial tile, but now clients have a set of linear fragments to iterate through instead of one enormous fragment. More detailed information is available in the example below. Retention policy . A retention policydetermines how long data will be kept and stored. Its purpose is to ensure the efficient use of storage resources by controlling data growth over time. Setting a retention policy per view to minimise storage fill-up is possible. Implementing a retention policy helps organisations maintain control over their data growth and ensure that storage resources are used optimally. The policy specifies the maximum duration that data should be kept. Time based retention policy . The time-based retention policy can be configured using the ISO 8601 duration format. This time-based policy ensures that data is automatically deleted after a specified period, freeing up valuable storage space for new data. @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix server: &lt;http://localhost:8080/mobility-hindrances/&gt; . server:time-based-retention tree:viewDescription [ ldes:retentionPolicy [ a ldes:DurationAgoPolicy ; tree:value \"PT5M\"^^xsd:duration ; ] ; ] . duration: “PT5M” . As an example, the time-based retention configuration example above is set up to ensure that data is automatically deleted after 5 minutes (PT5M). Point-in-time retention policy . The point in time retention policy of the Linked Data Event Stream (LDES) only preserves the members created after a specific moment. In this way, only the members made after a given point in time retain. @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix server: &lt;http://localhost:8080/mobility-hindrances/&gt; . server:point-in-time-retention tree:viewDescription [ ldes:retentionPolicy [ a ldes:PointInTimePolicy ; ldes:pointInTime \"2023-04-12T00:00:00\"^^xsd:dateTime ] ; ] . Version-based retention policy . The version-based retention policy of the system ensures that only the x most recent members of each state object are retained. In this way, only the x most recent members of each state object retain. @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix server: &lt;http://localhost:8080/mobility-hindrances/&gt; . server:version-based-retention tree:viewDescription [ ldes:retentionPolicy [ a ldes:LatestVersionSubset; ldes:amount 2 ; ] ; ] . Combine retention policies . Time based and Point-in-time retention . The integration of Time-based and Point-in-Time retention strategies offers a tactical method for organizations aiming to restrict data volume from a certain Point in Time within a specified time frame. . Time-based and Version-based retention . This can be used, when the focus is on datasets that are primarily relevant in their ‘latest state’. A practical example is a dataset like a view of an address registry, where only the latest, updated addresses are pertinent. Such datasets require a dynamic approach to data retention. Version-based retention: to retain only the most recent version of a data item. Time-based retention: to maintain changes within a specific time frame. Data users can retrieve and maintain the full state of the data, regardless of when they access the data stream, all clients have the same information, leading to uniformity and consistency in data analysis. On the other hand, for data publishers; there’s no need to store all historical data. Only the most recent state plus a short history (e.g., two days) is retained. This approach results in datasets that do not grow exponentially, simplifying management and accessibility. The combination of Version-based and Time-based Retention offers an advanced solution for managing dynamic datasets. This approach is particularly suited for scenarios where only the latest data is relevant, while still retaining a limited historical context. It achieves an optimal balance between accessibility, consistency, and efficiency in both data storage and usage. Hosting the LDES stream SHACL shape . SHACL (Shapes Constraint Language) is a language used to validate RDF graphs against a set of conditions provided as shapes and other constructs in an RDF graph. The LDES Server facilitates hosting a SHACL shape describing the members in the LDES. Through configuration, it is possible to reference an existing SHACL shape via an URL or to provide a static file with an RDF description of the SHACL shape. Hosting DCAT metadata . DCAT is a standardised RDF vocabulary to describe data catalogues on the Web, allowing easy interoperability between catalogues. Using a standard schema, DCAT enhances discoverability and facilitates federated search across multiple catalogues. The LDES server facilitates hosting DCAT metadata when publishing an LDES. Through configuration, as with the SHACL shape, it is possible to reference an existing DCAT via an URI or to provide a static file containing an RDF description of the DCAT. More information on configuring DCAT on the LDES Server can be found here. Add DCAT configuration for the LDES server . @prefix dct: &lt;http://purl.org/dc/terms/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . [] a dcat:Catalog ; dct:title \"My LDES'es\"@en ; dct:description \"All LDES'es from publiser X\"@en . Add DCAT metadata for a LDES . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix dc: &lt;http://purl.org/dc/terms/&gt; . [] a dcat:Dataset ; dc:title \"My LDES\"@en ; dc:description \"LDES for my data collection\"@en . ",
    "url": "/publisher/LDES_server#setting-up-the-ldes-server-during-startup-process",
    
    "relUrl": "/publisher/LDES_server#setting-up-the-ldes-server-during-startup-process"
  },"68": {
    "doc": "5. LDES server",
    "title": "Setting up the LDES Server using API",
    "content": "Setup of the LDES Server . To start a default LDES Server, a few basic steps are needed. | Create a ldes-server.yml config file with this basic content | . mongock: migration-scan-package: VSDS springdoc: swagger-ui: path: /v1/swagger ldes-server: host-name: \"http://localhost:8080\" management: tracing: enabled: false spring: data: mongodb: database: ldes host: ldes-mongodb port: 27017 auto-index-creation: true . | Create a local docker-compose.yml file with the content below. | . version: \"3.3\" services: ldes-server: container_name: basic_ldes-server image: ghcr.io/informatievlaanderen/ldes-server:20230602200451 environment: - SPRING_CONFIG_LOCATION=/config/ volumes: - ./ldes-server.yml:/config/application.yml:ro ports: - 8080:8080 networks: - ldes depends_on: - ldes-mongodb ldes-mongodb: container_name: quick_start_ldes-mongodb image: mongo:6.0.4 ports: - 27017:27017 networks: - ldes networks: ldes: name: quick_start_network . | Run docker compose up within the work directory of docker-compose.yml file to start the containers. | . Setting up metadata for the server . Setting up metadata for your LDES Server can be done by posting a RDF object defining a DCAT catalog to /admin/api/v1/dcat . @prefix dct: &lt;http://purl.org/dc/terms/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; . @prefix org: &lt;http://www.w3.org/ns/org#&gt; . @prefix legal: &lt;http://www.w3.org/ns/legal#&gt; . @prefix m8g: &lt;http://data.europa.eu/m8g/&gt; . @prefix locn: &lt;http://www.w3.org/ns/locn#&gt; . [] a dcat:Catalog ; dct:title \"My LDES'es\"@en ; dct:description \"All LDES'es from publiser X\"@en ; dct:publisher &lt;http://sample.org/company/PublisherX&gt; . &lt;http://sample.org/company/PublisherX&gt; a legal:LegalEntity ; foaf:name \"Data Publishing Company\" ; legal:legalName \"Data Publishing Company BV\" ; m8g:registeredAddress [ a locn:Address ; locn:fullAddress \"Some full address here\" ] ; m8g:contactPoint [ a m8g:ContactPoint ; m8g:hasEmail \"info@data-publishing-company.com\" ] . This can be updated by performing a PUT operation with an updated DCAT catalog on /admin/api/v1/dcat/{catalogID} . Finally, to delete the catalog, a DELETE request can be performed at /admin/api/v1/dcat/{catalogID} . Further documentation can be found on the internal Swagger API available at /v1/swagger . Setting up a collection . Setting up a collection on the LDES Server can be done by posting a RDF object defining a collection to /admin/api/v1/eventstreams . @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix custom: &lt;http://example.org/&gt; . @prefix dcterms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix sh: &lt;http://www.w3.org/ns/shacl#&gt; . @prefix server: &lt;http://localhost:8080/&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . server:exampleCollection a ldes:EventStream ; ldes:timestampPath dcterms:created ; ldes:versionOfPath dcterms:isVersionOf ; custom:memberType &lt;https://data.vlaanderen.be/ns/mobiliteit#Mobiliteitshinder&gt; ; custom:hasDefaultView \"true\"^^xsd:boolean ; tree:shape [ sh:closed \"true\"; a sh:NodeShape ; ] . This collection can be deleted by performing a DELETE request on /admin/api/v1/eventstreams/{collectionName} . Further documentation can be found on the internal Swagger API available at /v1/swagger . Setting up metadata for collection . To add metadata to an inserted collection, one can post a DCAT dataset on /admin/api/v1/eventstreams/{collectionName}/dcat . @prefix dct: &lt;http://purl.org/dc/terms/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; . @prefix org: &lt;http://www.w3.org/ns/org#&gt; . @prefix legal: &lt;http://www.w3.org/ns/legal#&gt; . @prefix m8g: &lt;http://data.europa.eu/m8g/&gt; . @prefix locn: &lt;http://www.w3.org/ns/locn#&gt; . [] a dcat:Dataset ; dct:title \"My LDES\"@en ; dct:title \"Mijn LDES\"@nl ; dct:description \"LDES for my data collection\"@en ; dct:description \"LDES vir my data-insameling\"@af ; dct:creator &lt;http://sample.org/company/MyDataOwner&gt; . &lt;http://sample.org/company/MyDataOwner&gt; a legal:LegalEntity ; foaf:name \"Data Company\" ; legal:legalName \"Data Company BV\" ; m8g:registeredAddress [ a locn:Address ; locn:fullAddress \"My full address here\" ] ; m8g:contactPoint [ a m8g:ContactPoint ; m8g:hasEmail \"info@data-company.com\" ] . To update this entry, a PUT request can be performed on /admin/api/v1/eventstreams/{collectionName}/dcat. Similarly, a DELETE request can be performed on /admin/api/v1/eventstreams/{collectionName}/dcat . Further documentation can be found on the internal Swagger API available at /v1/swagger . Setting up a view . Setting up a view on the LDES Server can be done by performing a POST operation with a RDF object defining a collection to /admin/api/v1/eventstreams/{collectionName}/views . @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix example: &lt;http://example.org/&gt; . @prefix server: &lt;http://localhost:8080/name1/&gt; . @prefix viewName: &lt;http://localhost:8080/name1/view1/&gt; . viewName:description a &lt;https://w3id.org/tree#ViewDescription&gt; ; ldes:retentionPolicy [ a ldes:retentionPolicy ; example:name \"timebased\"; example:duration \"10\" ; ] . server:view1 &lt;https://w3id.org/tree#viewDescription&gt; &lt;http://localhost:8080/name1/view1/description&gt; . Further documentation can be found on the internal Swagger API available at /v1/swagger . Setting up metadata for view . To add metadata to an inserted view, one can perform a PUT operation with a DCAT view description and dataservice on /admin/api/v1/eventstreams/{collectionName}/views/{viewName}/dcat . @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix dc: &lt;http://purl.org/dc/terms/&gt; . @prefix host: &lt;http://localhost:8080/&gt; . @prefix server: &lt;http://localhost:8080/collection/&gt; . @prefix viewName: &lt;http://localhost:8080/collection/viewName/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix skos: &lt;http://www.w3.org/2004/02/skos/core#&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . server:viewName a tree:Node ; tree:viewDescription viewName:description . viewName:description a dcat:DataService , tree:ViewDescription ; tree:fragmentationStrategy ([ a tree:ExampleFragmentation ; tree:pageSize \"100\" ; tree:property \"example/property\" ]) ; dc:description \"Geospatial fragmentation for my LDES\"@en ; dc:title \"My geo-spatial view\"@en ; dc:license [ a dc:LicenseDocument ; dc:type [ a skos:Concept ; skos:prefLabel \"some public license\"@en ] ] ; ldes:retentionPolicy [ a ldes:DurationAgoPolicy ; tree:value \"PT2M\"^^xsd:duration ; ] ; dcat:endpointURL server:viewName ; dcat:servesDataset host:collection ; . Similarly, a DELETE request can be performed on /admin/api/v1/eventstreams/{collectionName}/views/{viewName}/dcat . Further documentation can be found on the internal Swagger API available at /v1/swagger . Setting up ACM/IDM . The Access and User Management (ACM) and Identity Management (IDM) of the Flemish government are products that allow you to manage the access and identity of data users that consume the published LDES. This ADM/IDM security option through an API gateway, protects LDES Collections and Views from unauthorized access, recognizing the importance of data security. The API gateway serves as a security layer, managing access and applying authentication methods, such as ACM/IDM, reducing the chance of exposing sensitive data to unwanted parties. ACM/IDM verifies the identity and permissions of users and devices. This improved security feature increases the trust and dependability of LDES Server for organizations working in security-sensitive environments. During the ACM/IDM Standard integration process, you will work alongside the ACM/IDM integration team of the Flemish government to follow the standard connection procedure. One of our analysts will guide you through this process via an integration file, where agreements and requirements are documented. This procedure is followed for both new files and modifications to existing files. You can find here more information. OpenAPI swagger UI . Via the OpenAPI Specification it becomes possible discover how the LDES server API works, how to configure the LDES server, etc., in a user-friendly manner. As an example, the Swagger API docs can be find here. The Swagger API should look like this: . | . ",
    "url": "/publisher/LDES_server#setting-up-the-ldes-server-using-api",
    
    "relUrl": "/publisher/LDES_server#setting-up-the-ldes-server-using-api"
  },"69": {
    "doc": "5. LDES server",
    "title": "5. LDES server",
    "content": " ",
    "url": "/publisher/LDES_server",
    
    "relUrl": "/publisher/LDES_server"
  },"70": {
    "doc": "2. Adapter components",
    "title": "Adapter components",
    "content": "The Adapter components are components to be used in conjunction with the input components. These components transform the provided content into and internal Linked Data model and sends it down the pipeline. ",
    "url": "/publisher/adapters/index#adapter-components",
    
    "relUrl": "/publisher/adapters/index#adapter-components"
  },"71": {
    "doc": "2. Adapter components",
    "title": "2. Adapter components",
    "content": " ",
    "url": "/publisher/adapters/index",
    
    "relUrl": "/publisher/adapters/index"
  },"72": {
    "doc": "Json To JsonLd Transformer",
    "title": "Json To JsonLd Transformer",
    "content": "LDIO Component Name: Ldio:JsonToLdAdapter see reference guide Apache Nifi Component Name: Json to Json LD Processor see reference guide . The json-to-ld-adapter receives json messages and adds a linked data context to transform the messages to json-ld. The JSON-to-LD Adapter is designed to bridge the gap between conventional JSON messages and the more semantically rich JSON-LD (JSON for Linked Data). At its core, this adapter takes incoming JSON messages, which are widely used for their simplicity and flexibility in data interchange, and enhances them by appending a linked data context. graph LR L[JSON] --&gt; H[Json-to-JsonLD-Transformer] H --&gt; S[Json-LD] subgraph Publishing Pipeline H end . ",
    "url": "/publisher/adapters/json-to-json-ld",
    
    "relUrl": "/publisher/adapters/json-to-json-ld"
  },"73": {
    "doc": "NGSI V2 to LD Adapter",
    "title": "NGSI v2 to LD Adapter",
    "content": "LDIO Component Name: Ldio:NgsiV2ToLdAdapter see reference guide Apache Nifi Component Name: NgsiV2ToLdAdapter Processor see reference guide . This adapter will transform a NGSI V2 input into NGSI LD. NGSI v2 is intended to manage the entire lifecycle of context information, including updates, queries, registrations, and subscriptions. NGSI v2 is a technical standard of Fiware designed for managing the context information lifecycle within smart solutions, particularly those related to smart cities or the Internet of Things (IoT). Context information refers to any data that can be used to characterize the state of an entity such as a device, user, or environment. The purpose of NGSI v2 is to provide a uniform way to handle this data, making it easier to develop, deploy, and interoperate IoT and smart applications. NGSI-LD represents an evolution from NGSI v2, by integrating linked data principles to improve interoperability and data sharing in IoT and smart environments. While NGSI v2 focuses on managing the lifecycle of context information through updates, queries, registrations, and subscriptions in a straightforward manner, NGSI-LD adds a layer of semantic richness and structure. It leverages standardized vocabularies and ontologies, enabling systems to not only exchange data but to understand its meaning across diverse domains. This is achieved through a graph-based representation of context information, allowing for complex relationships and structures to be efficiently navigated and interpreted. Key advancements include the use of property-of-property annotations and the management of temporal data, enhancing the standard’s capability to handle sophisticated scenarios and offering a comprehensive solution for data integration and interpretation in smart applications. Jackson is used to first deserialize the input to java objects which can then be serialized to the LD format. graph LR L[NGSIv2] --&gt; H[Adapter] H --&gt; S[Linked Data] subgraph LD Pipeline H end . ",
    "url": "/publisher/adapters/ngsiv2-to-ld#ngsi-v2-to-ld-adapter",
    
    "relUrl": "/publisher/adapters/ngsiv2-to-ld#ngsi-v2-to-ld-adapter"
  },"74": {
    "doc": "NGSI V2 to LD Adapter",
    "title": "Notes",
    "content": "The algorithm applies several deviations from the standard formats. These deviations are: . | The observedAt attribute is added to every property, its value is determined by the dateObserved attribute of the input. | The timestamp attribute of a metadata property normally determines the observedAt property but is ignored in this algorithm. | . ",
    "url": "/publisher/adapters/ngsiv2-to-ld#notes",
    
    "relUrl": "/publisher/adapters/ngsiv2-to-ld#notes"
  },"75": {
    "doc": "NGSI V2 to LD Adapter",
    "title": "NGSI V2 to LD Adapter",
    "content": " ",
    "url": "/publisher/adapters/ngsiv2-to-ld",
    
    "relUrl": "/publisher/adapters/ngsiv2-to-ld"
  },"76": {
    "doc": "RDF Adapter",
    "title": "RDF Adapter",
    "content": "LDIO Component Name: Ldio:RDFAdapter see reference guide Apache Nifi Component Name: RDF serialisation Processor see reference guide . As the most basic Adapter of the LDI Core Building Blocks, the RDF Adapter will take in an RDF string and convert it into another RDF serialisation. Following RDF serializations are supported: . RDF/XML, Turtle (Terse RDF Triple Language), N-Triples, N-Quads, JSON-LD (JSON for Linked Data), RDFa (RDF in attributes). graph LR L[RDF] --&gt; H[RDF adapter] H --&gt; S[RDF] subgraph LD Pipeline H end . ",
    "url": "/publisher/adapters/rdf-adapter",
    
    "relUrl": "/publisher/adapters/rdf-adapter"
  },"77": {
    "doc": "RML Adapter",
    "title": "RML Adapter",
    "content": "LDIO Component Name: Ldio:RmlAdapter see reference guide Apache Nifi Component Name: There is currently no RML adapter processor for Apache Nifi. The RML Adapter allows a user to transform a non-LD object (json/CSV/XML) to an RDF object using RML. This is done by providing a RML mapping file. An RML mapping file is essentially a blueprint that defines how elements within the source data (whether they be JSON properties, CSV columns, or XML elements) correlate with RDF triples’ subjects, predicates, and objects. These triples form the backbone of RDF’s graph-based model, representing semantic relationships between entities in a way that is both machine-readable and semantically rich. However, as RML is written in RDF, it can be challenging for a new user to create a new mapping. That’s where YARRRML comes into play. Along with the online editor Matey, it’s easy to build one’s own mapping and then export it into RML. graph LR L[non-linked data] --&gt; H[RML Adapter] H --&gt; S[Linked Data] subgraph Publishing pipeline H end . ",
    "url": "/publisher/adapters/rml-adapter",
    
    "relUrl": "/publisher/adapters/rml-adapter"
  },"78": {
    "doc": "Examples publishing Pipeline",
    "title": "Examples publishing pipelines",
    "content": ". Apache Kafka, Fiware-Orien Context Broker, and MQTT can be used in the publishing pipeline to publish data as LDES. ",
    "url": "/publisher/examples#examples-publishing-pipelines",
    
    "relUrl": "/publisher/examples#examples-publishing-pipelines"
  },"79": {
    "doc": "Examples publishing Pipeline",
    "title": "Kafka to LDES server",
    "content": "Apache Kafka can be used as a data provider for ingesting data topics into the LDES ecosystem. The diagram below illustrates how the VSDS NIFI solution subscribes to a Kafka stream, updates the dataset’s attributes, converts it into an LDES-formatted stream, and uses HTTP protocols to publish it to the LDES Server. This process enables fragmentation or pagination of the data. This GitHub repository demonstrates the configuration of transferring subscribed GRAR (Building units, addresses &amp; parcels) Kafka data stream to the published substring fragmented LDES Stream using LDES Server. As we have no control over the GRAR system, the demo uses a JSON Data Generator which produces a continuous stream of addresses as an alternative to the GRAR system. Also, the Apache NIFI has standard Kafka Reader processors for subscribing to Kafka stream, please modify the nifi-workflow.json accordingly based on your environment. An example setup with Kafka can be as follow (GRAR.json), please modify the credentials for the Kafka topic accordingly: . To try out the demo, you need to make sure the required ports for LDES Server and NIFI are free to be used. For the details, please refer to docker-compose.yml. The steps are composed of the following steps: . 1. Docker run start all required docker images. 2. Upload a pre-defined NiFi workflow. 3. Start the NiFi workflow. 4. Start the address ingestion. (Modify according to your Kafka setup) . Please follow README.md for step-by-step guide. ",
    "url": "/publisher/examples#kafka-to-ldes-server",
    
    "relUrl": "/publisher/examples#kafka-to-ldes-server"
  },"80": {
    "doc": "Examples publishing Pipeline",
    "title": "MQTT to LDES server",
    "content": ". ",
    "url": "/publisher/examples#mqtt-to-ldes-server",
    
    "relUrl": "/publisher/examples#mqtt-to-ldes-server"
  },"81": {
    "doc": "Examples publishing Pipeline",
    "title": "Fiware to LDES server",
    "content": "The FIWARE-Orion Context Broker (OCB) can be integrated as a data provider with the VSDS LDES (Linked Data Event Streams) Server. The OCB is an open-source software component developed by FIWARE that can manage real-time context information by receiving updates from IoT devices, sensors, and other sources and storing this information in a centralized location. One example of this integration is demonstrated in the diagram below, which illustrates the use case of onboarding the Internet of Water (VMM) data. In this case, the OCB is integrated into the LDES ecosystem to publish context updates to an LDES stream. The VSDS NIFI solution is used to translate the context data into LDES events and publish them to the LDES stream via an update attributes processor, an OSLO converter processor, and an LdesConverter process NIFI pipeline. Once the context updates are published to the LDES Sever in LDES formatted stream, they can be processed and stored in the LDES Server as linked data. This makes the context information available for further analysis and uses in other systems. ",
    "url": "/publisher/examples#fiware-to-ldes-server",
    
    "relUrl": "/publisher/examples#fiware-to-ldes-server"
  },"82": {
    "doc": "Examples publishing Pipeline",
    "title": "Examples publishing Pipeline",
    "content": " ",
    "url": "/publisher/examples",
    
    "relUrl": "/publisher/examples"
  },"83": {
    "doc": "Publishing Pipeline",
    "title": "Publishing Pipeline",
    "content": ". A data publisher publishes its data in the Flanders Smart Data Space as a Linked Data Event Stream (LDES). In order to do so, a publishing pipeline needs to be configured. Without going too deeply into the specific configuration of the necessary building blocks, this part assists you in how this publishing pipeline needs to look like. After this, you can go directly to the tutorials, with the basic concepts and background knowledge in your back pocket. For instance, when a data publisher aims to distribute non-linked data as an LDES, a specifically configured publisher pipeline manages the entire process. Initially, an adapter component transforms the data into linked data. Subsequently, a transformer component converts the geometry into WKT format. Once the data is prepared for publication, it is transmitted to the LDES server using an HTTP-out component. It’s important to note that the LDES server is not part of the pipeline; rather, it functions as a separate VSDS building block. For effective data publication, two services need to be deployed. ",
    "url": "/publisher/index",
    
    "relUrl": "/publisher/index"
  },"84": {
    "doc": "AMQP In",
    "title": "AMQP broker message In",
    "content": "LDIO Component Name: Ldio:AmqpIn see reference guide Apache Nifi Component Name: ConsumeAMQP see Apache Nifi reference guide . Consumes AMQP Messages from an AMQP Broker using the AMQP protocol. Each message that is received from the AMQP Broker will be emitted downstream the pipeline. The LDIO AMQP In listens to messages from an AMQP 1.0 queue. graph LR L[AMQP broker] --&gt; H[AMQP message In] H --&gt; S[LDES server] subgraph Publishing Pipeline H end . ",
    "url": "/publisher/inputs/amqp-in#amqp-broker-message-in",
    
    "relUrl": "/publisher/inputs/amqp-in#amqp-broker-message-in"
  },"85": {
    "doc": "AMQP In",
    "title": "AMQP In",
    "content": " ",
    "url": "/publisher/inputs/amqp-in",
    
    "relUrl": "/publisher/inputs/amqp-in"
  },"86": {
    "doc": "HTTP In Poller",
    "title": "HTTP In Poller",
    "content": "LDIO Component Name: Ldio:LdioHttpInPollersee reference guide . Apache Nifi Component Name: InvokeHTTP see reference guide . The LDIO HTTP In Poller is a basic HTTP Poller that will poll a target URL on a specified interval. This component fetches data from an HTTP endpoint at a configured interval. It is designed to process input in various content types, including XML (text/xml, application/xml), JSON (application/json), and RDF (text/turtle, application/ld+json, application/n-quads, application/n-triples, application/rdf+xml). The expected output of the component is in these same formats, supporting XML, JSON, and RDF content types. graph LR L[HTTP endpoint] --&gt; H[HTTP poller component] H --&gt; S[...] subgraph Publishing Pipeline H end . ",
    "url": "/publisher/inputs/http-in-poller",
    
    "relUrl": "/publisher/inputs/http-in-poller"
  },"87": {
    "doc": "HTTP In",
    "title": "HTTP In",
    "content": "LDIO Component Name: Ldio:LdioHttpIn see reference guide . Apache Nifi Component Name: InvokeHTTP see Apache Nifi reference guide . The LDIO HTTP In is a basic HTTP Listener. This component listens for HTTP messages at the endpoint http://{hostname}:{port}/{pipeline name}. It supports processing input in various content types, including XML (text/xml, application/xml), JSON (application/json), and RDF (text/turtle, application/ld+json, application/n-quads, application/n-triples, application/rdf+xml). The expected output of this component is also in similar formats, supporting XML, JSON, and RDF content types. graph LR L[endpoint HTTP messages] --&gt; H[Http in Listener] H --&gt; S[...] subgraph Publishing Pipeline H end . ",
    "url": "/publisher/inputs/http-in",
    
    "relUrl": "/publisher/inputs/http-in"
  },"88": {
    "doc": "1. Input components",
    "title": "Input components",
    "content": "The Input components are components that will receive data (not necessarily LD) to then feed the publishing pipeline. ",
    "url": "/publisher/inputs/index#input-components",
    
    "relUrl": "/publisher/inputs/index#input-components"
  },"89": {
    "doc": "1. Input components",
    "title": "1. Input components",
    "content": " ",
    "url": "/publisher/inputs/index",
    
    "relUrl": "/publisher/inputs/index"
  },"90": {
    "doc": "Kafka In",
    "title": "Kafka In",
    "content": "LDIO Component Name: Ldio:LdioKafkaIn see reference guide Apache Nifi Component Name: ConsumeKafka see Apache Nifi reference guide . The LDIO Kafka In component is vital to the Publishing Pipeline, specifically designed to interact with Kafka, a distributed event streaming platform. This component is responsible for listening to messages from a specified kafka topic, which is crucial in integrating with an LDES server. graph LR L[Kafka topic] --&gt; H[Kafka in component] H --&gt; S[LDES server] subgraph Publishing Pipeline H end . ",
    "url": "/publisher/inputs/kafka-in",
    
    "relUrl": "/publisher/inputs/kafka-in"
  },"91": {
    "doc": "Console Out",
    "title": "LDIO Console Out",
    "content": "LDIO Component Name: Ldio:ConsoleOut see reference guide Apache Nifi Component Name: LogMessage see Apache Nifi reference guide . The LDIO Console Out will output its given model to the console. In Apache Nifi, LogMessage processor, will emit a log message at the specified log level. ",
    "url": "/publisher/outputs/console-out#ldio-console-out",
    
    "relUrl": "/publisher/outputs/console-out#ldio-console-out"
  },"92": {
    "doc": "Console Out",
    "title": "Console Out",
    "content": " ",
    "url": "/publisher/outputs/console-out",
    
    "relUrl": "/publisher/outputs/console-out"
  },"93": {
    "doc": "HTTP Out",
    "title": "LDIO HTTP Out",
    "content": "LDIO Component Name: Ldio:HttpOut see reference guide Apache Nifi Component Name: InvokeHTTP see Apache Nifi reference guide . The LDIO HTTP Out is a basic Http Client that will send the given Linked Data model to a target url. This pipeline component is responsible for sending harvested LDES members to an external destination (url) using the HTTP (Hypertext Transfer Protocol). HTTP is the foundational protocol used for transmitting data over the internet, primarily used for loading web pages in a browser, but it’s also widely utilized in various other types of network communication. graph LR LDES --&gt; C[Client] C --&gt; H[LDIO HTTP out] H --&gt; S[url] subgraph Publishing Pipeline C H end . ",
    "url": "/publisher/outputs/http-out#ldio-http-out",
    
    "relUrl": "/publisher/outputs/http-out#ldio-http-out"
  },"94": {
    "doc": "HTTP Out",
    "title": "HTTP Out",
    "content": " ",
    "url": "/publisher/outputs/http-out",
    
    "relUrl": "/publisher/outputs/http-out"
  },"95": {
    "doc": "4. Output components",
    "title": "Output components",
    "content": "The Output components are components that will take in Linked Data and will export it to external sources. ",
    "url": "/publisher/outputs/index#output-components",
    
    "relUrl": "/publisher/outputs/index#output-components"
  },"96": {
    "doc": "4. Output components",
    "title": "4. Output components",
    "content": " ",
    "url": "/publisher/outputs/index",
    
    "relUrl": "/publisher/outputs/index"
  },"97": {
    "doc": "Kafka Out",
    "title": "Kafka Out",
    "content": "LDIO Component Name: Ldio:LdioKafkaIn see reference guide Apache Nifi Component Name: ConsumeKafka see Apache Nifi reference guide . The Kafka Out sends messages to a kafka topic. graph LR LDES --&gt; C[Client] C --&gt; H[LDIO Kafka Out Component] H --&gt; S[Kafka topic] subgraph Publishing Pipeline C H end . ",
    "url": "/publisher/outputs/kafka-out",
    
    "relUrl": "/publisher/outputs/kafka-out"
  },"98": {
    "doc": "Repository Materialization",
    "title": "Repository Materialiser",
    "content": "LDIO Component Name: Ldio:RepositoryMaterialiser see reference guide Apache Nifi Component Name: RepositoryMaterialiser see reference guide . The repository materialiser is used to materialise an LDES stream into a triplestore. Any triplestore that supports the RDF4J remote repository API can be used. graph LR L[LDES members] --&gt; H[Repository Materialiser] H --&gt; S[Tripple Store] subgraph Publishing Pipeline H end . Batching . To increase the performance of this materialiser, members will be committed in batch to the triple store. However, it’s important to notice that this can have an impact on the data integrity. First of all, there could be a delay, with a maximum delay of the configured batch timeout, when the triple store will be up-to-date. Secondly, if something goes wrong halfway of a batch, all the members in that batch will not be committed to triple story and thus will be gone. So the more important data integrity is, the lower the batch-size and batch-timeout should be configured. If a more performant repository materialiser is desired, batch-size and batch-timeout should be configured somewhat higher. ",
    "url": "/publisher/outputs/repository-materialiser#repository-materialiser",
    
    "relUrl": "/publisher/outputs/repository-materialiser#repository-materialiser"
  },"99": {
    "doc": "Repository Materialization",
    "title": "Repository Materialization",
    "content": " ",
    "url": "/publisher/outputs/repository-materialiser",
    
    "relUrl": "/publisher/outputs/repository-materialiser"
  },"100": {
    "doc": "GeoJson to Wkt Transformer",
    "title": "GeoJson to Wkt Transformer",
    "content": "LDIO Component Name: Ldio:GeoJsonToWktTransformer see reference guide Apache Nifi Component Name: GeoJson-to-Wkt-Transformer see Apache Nifi reference guide . The GeoJson to Wkt Transformer will transform any [GeoJson] statements (with predicate https://purl.org/geojson/vocab#geometry) to a [wkt string][WKT]. graph LR L[GeoJson] --&gt; H[GeoJson To WKT Transformer] H --&gt; S[WKT] subgraph Publishing Pipeline H end . ",
    "url": "/publisher/transformers/geojson-to-wkt",
    
    "relUrl": "/publisher/transformers/geojson-to-wkt"
  },"101": {
    "doc": "Http Enricher Transformer",
    "title": "Http Enricher",
    "content": "LDIO Component Name: Ldio:HttpEnricher see reference guide Apache Nifi Component Name: ... see reference guide . A transformer that allows sending a GET or POST HTTP request to a dynamic URL provided by the model. The response is converted to linked data and added to the incoming model. graph LR L[GET/POST] --&gt; H[Http Enricher] H --&gt; S[Linked Data response] subgraph Publishing Pipeline H end . ",
    "url": "/publisher/transformers/http-enricher#http-enricher",
    
    "relUrl": "/publisher/transformers/http-enricher#http-enricher"
  },"102": {
    "doc": "Http Enricher Transformer",
    "title": "Http Enricher Transformer",
    "content": " ",
    "url": "/publisher/transformers/http-enricher",
    
    "relUrl": "/publisher/transformers/http-enricher"
  },"103": {
    "doc": "3. Transformer components",
    "title": "Transformer components",
    "content": "The Transformer components are components that takes in a Linked Data model, transform/modify it and then put it back on the publishing pipeline. ",
    "url": "/publisher/transformers/index#transformer-components",
    
    "relUrl": "/publisher/transformers/index#transformer-components"
  },"104": {
    "doc": "3. Transformer components",
    "title": "3. Transformer components",
    "content": " ",
    "url": "/publisher/transformers/index",
    
    "relUrl": "/publisher/transformers/index"
  },"105": {
    "doc": "SPARQL Construct",
    "title": "SPARQL Construct",
    "content": "LDIO Component Name: Ldio:SparqlConstructTransformer see reference guide . Apache Nifi Component Name: SPARQL Interactions Processors see reference guide . The SPARQL Construct Transformer will modify the model based on the given SPARQL Construct Query. SPARQL Construct is a query language used in semantic Web technologies to create RDF (Resource Description Framework) graphs from existing RDF data. It allows users to specify a pattern of data they wish to extract from the RDF data and construct a new graph based on that pattern. The SPARQL Construct query language provides a powerful way to create new RDF data by using existing data as the input. It can be used to transform RDF data into different formats, as well as to simplify the structure of RDF data by aggregating or filtering data. This SPARQL Construct Transfomer building block can be used to execute model transformations. ",
    "url": "/publisher/transformers/sparql-construct",
    
    "relUrl": "/publisher/transformers/sparql-construct"
  },"106": {
    "doc": "SPARQL Construct",
    "title": "Splitting models using SPARQL Construct",
    "content": "This component can be used to split models into multiple models using graphs. For example, the below query will create a dataset containing multiple models defined by ‘GRAPH’. The SPARQL construct component will extract all named models from the dataset and add all statements from the default model. The component will then return a collection of models. CONSTRUCT { GRAPH ?s { ?s ?p ?o } } WHERE { ?s ?p ?o } . ",
    "url": "/publisher/transformers/sparql-construct#splitting-models-using-sparql-construct",
    
    "relUrl": "/publisher/transformers/sparql-construct#splitting-models-using-sparql-construct"
  },"107": {
    "doc": "SPARQL Construct",
    "title": "SPARQL functions",
    "content": "We support some additional geo functions that can call inside your SPARQL Construct query, . with the following namespace: . prefix geoc: https://opengis.net/def/function/geosparql/custom# . | Function | Description | Input | Output | . | geoc:lineAtIndex | get LineString from MultiLineString by index. | MultiLineString(wktLiteral) &amp; index | LineString(wktLiteral) | . | geoc:firstCoordinate | get first Coordinate of LineString. | LineString(wktLiteral) | Coordinate(wktLiteral) | . | geoc:lastCoordinate | get last Coordinate of LineString. | LineString(wktLiteral) | Coordinate(wktLiteral) | . | geoc:lineLength | calculate total line length of LineString. | LineString(wktLiteral) | distance in meters | . | geoc:midPoint | calculate midpoint of LineString. | LineString(wktLiteral) | Coordinate(wktLiteral) | . | geoc:pointAtFromStart | calculate point on LineString by distance. | LineString(wktLiteral) &amp; distance in meters | Coordinate(wktLiteral) | . |   |   |   |   | . ",
    "url": "/publisher/transformers/sparql-construct#sparql-functions",
    
    "relUrl": "/publisher/transformers/sparql-construct#sparql-functions"
  },"108": {
    "doc": "Version Materializer",
    "title": "Version Materializer",
    "content": "LDIO Component Name: Ldio:VersionMaterialiser see reference guide Apache Nifi Component Name: VersionMaterialiser see reference guide . The Version Materializer will transform a Version Object to a State Object. ",
    "url": "/publisher/transformers/version-materializer",
    
    "relUrl": "/publisher/transformers/version-materializer"
  },"109": {
    "doc": "Version Object Creator",
    "title": "Version Object Creator",
    "content": "LDIO Component Name: Ldio:VersionObjectCreator see reference guide Apache Nifi Component Name: VersionObjectCreator see reference guide . The Version Object Creator will transform a State Object to a Version Object. ",
    "url": "/publisher/transformers/version-object-creator",
    
    "relUrl": "/publisher/transformers/version-object-creator"
  },"110": {
    "doc": "2. Adapter components",
    "title": "Adapters components",
    "content": "The Adapter components are components to be used in conjunction with the LDI Input, the LDI Adapter will transform the provided content into and internal Linked Data model and sends it down the pipeline. ",
    "url": "/consumer/adapters/index#adapters-components",
    
    "relUrl": "/consumer/adapters/index#adapters-components"
  },"111": {
    "doc": "2. Adapter components",
    "title": "2. Adapter components",
    "content": " ",
    "url": "/consumer/adapters/index",
    
    "relUrl": "/consumer/adapters/index"
  },"112": {
    "doc": "RDF Adapter",
    "title": "RDF Adapter",
    "content": "LDIO Component Name: Ldio:RdfAdapter see reference guide Apache Nifi Component Name: there is no Apache Nifi variant created within the frame of VSDS . As the most basic Adapter of the LDI Core Building Blocks, the RDF Adapter will take in an RDF string and convert it into an internal Linked Data model. ",
    "url": "/consumer/adapters/rdf-adapter",
    
    "relUrl": "/consumer/adapters/rdf-adapter"
  },"113": {
    "doc": "RDF Adapter",
    "title": "Notes",
    "content": "This Adapter only supports valid RDF mime types . graph LR L[...] --&gt; H[RDF writer] H --&gt; S[correct RDF] subgraph LD Pipeline H end . ",
    "url": "/consumer/adapters/rdf-adapter#notes",
    
    "relUrl": "/consumer/adapters/rdf-adapter#notes"
  },"114": {
    "doc": "Examples consuming Pipeline",
    "title": "Examples consuming pipelines",
    "content": "This sections provides an overview of how data consumer can replicate data to different backend systems. ",
    "url": "/consumer/examples#examples-consuming-pipelines",
    
    "relUrl": "/consumer/examples#examples-consuming-pipelines"
  },"115": {
    "doc": "Examples consuming Pipeline",
    "title": "Triple stores",
    "content": ". A vivid illustration of a consumption pipeline involves the process of harvesting members from a Linked Data Event Stream (LDES) and subsequently storing them in a triplestore. Once stored, these data points can be efficiently queried and analyzed using SPARQL queries. This pipeline not only showcases the seamless integration of data into a structured repository but also highlights the ease with which users can access and manipulate this information, thereby unlocking a wealth of insights and opportunities for exploration. You can find more information in this article . ",
    "url": "/consumer/examples#triple-stores",
    
    "relUrl": "/consumer/examples#triple-stores"
  },"116": {
    "doc": "Examples consuming Pipeline",
    "title": "PostgreSQL &amp; TimescaleDB",
    "content": ". You can find more information in this article . ",
    "url": "/consumer/examples#postgresql--timescaledb",
    
    "relUrl": "/consumer/examples#postgresql--timescaledb"
  },"117": {
    "doc": "Examples consuming Pipeline",
    "title": "PowerBI",
    "content": ". You can find more information in this article . ",
    "url": "/consumer/examples#powerbi",
    
    "relUrl": "/consumer/examples#powerbi"
  },"118": {
    "doc": "Examples consuming Pipeline",
    "title": "GeoServer",
    "content": ". You can find more information in this article . ",
    "url": "/consumer/examples#geoserver",
    
    "relUrl": "/consumer/examples#geoserver"
  },"119": {
    "doc": "Examples consuming Pipeline",
    "title": "LDES to QGIS",
    "content": ". You can find more information in this article . ",
    "url": "/consumer/examples#ldes-to-qgis",
    
    "relUrl": "/consumer/examples#ldes-to-qgis"
  },"120": {
    "doc": "Examples consuming Pipeline",
    "title": "Machine Learning (ML-LDES server)",
    "content": ". You can find more information in this and this article . git config –global core.autocrlf false . ",
    "url": "/consumer/examples#machine-learning-ml-ldes-server",
    
    "relUrl": "/consumer/examples#machine-learning-ml-ldes-server"
  },"121": {
    "doc": "Examples consuming Pipeline",
    "title": "Examples consuming Pipeline",
    "content": " ",
    "url": "/consumer/examples",
    
    "relUrl": "/consumer/examples"
  },"122": {
    "doc": "Consuming pipeline",
    "title": "Consuming Pipeline",
    "content": ". As a data consumer, you want to access and utilize data from the Flanders Smart Data Space. This data is offered in the form of Linked Data Event Streams. This section aims to guide you through the process of creating the perfect consumption pipeline, providing a broad overview without delving into the complexities of each pipeline component. Armed with this foundational knowledge, you can seamlessly transition to the tutorials, ready to apply the essential concepts and background information to your data consumption tasks. For instance, if a data user intends to access data from an LDES and store it in a database (such as GraphDB), they will need to establish a consuming data pipeline. Initially, the configuration of the LDES client component is required. This component, an integral part of the data pipeline, sequentially retrieves each LDES member and forwards them through the process. Following this, a Repository Materializer component must be integrated into the pipeline to enable the writing of LDES members to the GraphDB. ",
    "url": "/consumer/index#consuming-pipeline",
    
    "relUrl": "/consumer/index#consuming-pipeline"
  },"123": {
    "doc": "Consuming pipeline",
    "title": "Consuming pipeline",
    "content": " ",
    "url": "/consumer/index",
    
    "relUrl": "/consumer/index"
  },"124": {
    "doc": "1. Input components",
    "title": "Input components",
    "content": "The input components are components that will receive data (not necessarily) to then feed the LDI pipeline. ",
    "url": "/consumer/inputs/index#input-components",
    
    "relUrl": "/consumer/inputs/index#input-components"
  },"125": {
    "doc": "1. Input components",
    "title": "1. Input components",
    "content": " ",
    "url": "/consumer/inputs/index",
    
    "relUrl": "/consumer/inputs/index"
  },"126": {
    "doc": "LDES Client with Connector",
    "title": "Ldes Client Connector",
    "content": "LDIO connector name: Ldio:LdioLdesClientConnector see reference guide Apache Nifi Component Name: Ldio:LdioLdesClientConnector` see reference guide . This component adds EDC (Eclipse dataspace Connector) support to the ldio ldes client. If you’d like to know how to configure the LDES Client, we refer to the ldio ldes client. The additional functionality provided by this component makes it possible to use the Ldes Client to consume an LDES through an EDC connector. This component exposes two endpoints: . | http://://transfer The Ldio component will start the data transfer with the connector. You have to send the transfer request to the LdioLdesClientConnector instead of the EDC consumer connector. The LDIO Ldes Client Connector will start the transfer with the connector and also keep the transfer alive while consuming the LDES (e.g. request a new token when it expires). | http://://token This endpoint should never be called directly. This is the callback to be provided in the transfer request. The EDC connector will use this callback endpoint to provide the LDES Client with a token. | . ",
    "url": "/consumer/inputs/ldes-client-connector#ldes-client-connector",
    
    "relUrl": "/consumer/inputs/ldes-client-connector#ldes-client-connector"
  },"127": {
    "doc": "LDES Client with Connector",
    "title": "LDES Client with Connector",
    "content": " ",
    "url": "/consumer/inputs/ldes-client-connector",
    
    "relUrl": "/consumer/inputs/ldes-client-connector"
  },"128": {
    "doc": "LDES Client",
    "title": "LDES Client",
    "content": "LDIO Component Name: Ldio:LdioLdesClient see reference guide Apache Nifi Component Name: Ldio:LdioLdesClient see reference guide . The LDES Client stands as a critical component of the pipeline, arguably the most pivotal. The LDES Client is responsible for consuming members from an existing LDES, subsequently facilitating their smooth progression through the LDIO. The LDES Client contains the functionality to replicate and synchronize an LDES, and to persist its state for that process. This is achieved by configuring the processor with an initial fragment URL. When the processor is triggered, the fragment will be processed, and all relations will be added to the (non-persisted) queue.A queue that accepts new fragments to process is maintained as long as the processor runs. The processor also keeps track of the mutable and immutable fragments already processed. It will be ignored when an attempt is made to queue a known immutable fragment. Fragments in the mutable fragment store will be queued when they’re expired. Should a fragment be processed from a stream that does not set the max-age in the Cache-control header, a default expiration interval will be used to set an expiration date on the fragment. Processed members of mutable fragments are also kept in state. They are ignored if presented more than once. Within a fragment, members can be ordered based on a timestamp. The path to this timestamp has to be configured. If this path is missing, the members are ordered randomly. graph LR L[LDES] --&gt; H[LDES Client] H --&gt; S[...] S --&gt; Q[...] subgraph LD input Pipeline H S end . The LDES CLIENT is designed for replication and synchronisation, meaning the client can retrieve members of an LDES but also checks regularly if new members are added and fetch them, allowing data consumers to stay up to date with the dataset. To understand the functioning of an LDES client, it is important to understand how LDESes are published on the Web. The Linked Data Fragments principle is utilised for publishing an LDES, meaning that the data is published in one or more fragments and meaningful semantic links are created between these fragments. This approach facilitates clients to follow these links and discover additional data. However, the critical aspect for the LDES client is the notion of mutable and immutable fragments. When publishing an LDES stream, a common configuration is to have a maximum number of members per fragment. Once a fragment surpasses this limit, it is regarded as immutable, and a ‘Cache-control: immutable’ cache header is added to the fragment to signify this. This information is crucial for the LDES client since it only needs to retrieve an immutable fragment once, while mutable fragments must be regularly polled to identify new members. ",
    "url": "/consumer/inputs/ldes-client",
    
    "relUrl": "/consumer/inputs/ldes-client"
  },"129": {
    "doc": "LDES Client",
    "title": "Linked Data Interactions",
    "content": "The LDES client component is written in Java and available as an SDK in the Linked Data Interactions repository. More information about Linked Data Interactions can be found here. The LDES Client contains the functionality to replicate and synchornise an LDES, and to persist its state for that process. This is achieved by configuring the processor with an initial fragment URL. When the processor is triggered, the fragment will be processed, and all relations will be added to the (non-persisted) queue. As long as the processor runs, a queue that accepts new fragments to process is maintained. The processor also keeps track of the mutable and immutable fragments already processed. It will be ignored when an attempt is made to queue a known immutable fragment. Fragments in the mutable fragment store will be queued when they’re expired. Should a fragment be processed from a stream that does not set the max-age in the Cache-control header, a default expiration interval will be used to set an expiration date on the fragment. Processed members of mutable fragments are also kept in state. They are ignored if presented more than once. Within a fragment, members can be ordered based on a timestamp. The path to this timestamp has to be configured. If this path is missing, the members are ordered randomly. ",
    "url": "/consumer/inputs/ldes-client#linked-data-interactions",
    
    "relUrl": "/consumer/inputs/ldes-client#linked-data-interactions"
  },"130": {
    "doc": "5. LDES structure Discoverer",
    "title": "LDES structure Discoverer",
    "content": "A lightweight application that discovers the structure of an LDES or a view by retrieving all the tree node relations of that LDES or view. A use case for this could be when you are only interested in a part of the event stream. To know what part you can follow, the structure can be discovered first. ",
    "url": "/consumer/ldes-discoverer#ldes-structure-discoverer",
    
    "relUrl": "/consumer/ldes-discoverer#ldes-structure-discoverer"
  },"131": {
    "doc": "5. LDES structure Discoverer",
    "title": "Config",
    "content": "| Property | Description | Required | Default | Example | Supported values | . | url | Url where from the discoverer needs to start | Yes | N/A | http://example.com/my-api | HTTP and HTTPS url | . | source-format | The ‘Content-Type’ that should be requested to the server. | No | application/n-quads | text/turtle | Any type supported by Apache Jena | . | output-format | The RDF format that will be used to display all the found relations | No | text/turtle | application/ld+json | Any type supported by Apache Jena | . ",
    "url": "/consumer/ldes-discoverer#config",
    
    "relUrl": "/consumer/ldes-discoverer#config"
  },"132": {
    "doc": "5. LDES structure Discoverer",
    "title": "How to run",
    "content": "This tutorial will show how to use the discoverer in Docker. In this example, we will try to discover the structure of an event stream of Geomobility. For simplicity, we recommend passing the config as arguments, like shown below: . docker run ldes/ldes-discoverer --url=\"http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11\" . NOTE: when an url contains a &amp; symbol, which will be picked up by the shell as an operator. In this example, if the url was not wrapped in quotation marks, the shell will try to execute three different command, where the last two will be month=05 and day=11, which will fail of course. To resolve this, make sure the url is encapsulated in quotation marks. Example output . In the logging of the application, both the total number of relations and the relations itself will be displayed. This will be present as the last logging statement of the app and would look something like this: . 2024-02-08T14:26:25.279+01:00 INFO 48176 --- [ main] b.v.i.l.l.d.common.LdesDiscoverer : Total of 97 relations found for url http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11 @prefix prov: &lt;http://www.w3.org/ns/prov#&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . &lt;http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11&amp;hour=05&amp;pageNumber=3&gt; tree:relation [ rdf:type tree:Relation; tree:node &lt;http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11&amp;hour=05&amp;pageNumber=4&gt; ] . &lt;http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11&amp;hour=05&amp;pageNumber=2&gt; tree:relation [ rdf:type tree:Relation; tree:node &lt;http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11&amp;hour=05&amp;pageNumber=3&gt; ] . &lt;http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11&gt; tree:relation [ rdf:type tree:InBetweenRelation; tree:node &lt;http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11&amp;hour=00&gt;; tree:path prov:generatedAtTime; tree:value \"2023-05-11T00\" ]; tree:relation [ rdf:type tree:InBetweenRelation; tree:node &lt;http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11&amp;hour=05&gt;; tree:path prov:generatedAtTime; tree:value \"2023-05-11T05\" ]; [...] tree:relation [ rdf:type tree:InBetweenRelation; tree:node &lt;http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11&amp;hour=20&gt;; tree:path prov:generatedAtTime; tree:value \"2023-05-11T20\" ] . [...] &lt;http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11&amp;hour=05&gt; tree:relation [ rdf:type tree:Relation; tree:node &lt;http://ldes-server/observations/by-time?year=2023&amp;month=05&amp;day=11&amp;hour=05&amp;pageNumber=1&gt; ] . ",
    "url": "/consumer/ldes-discoverer#how-to-run",
    
    "relUrl": "/consumer/ldes-discoverer#how-to-run"
  },"133": {
    "doc": "5. LDES structure Discoverer",
    "title": "5. LDES structure Discoverer",
    "content": " ",
    "url": "/consumer/ldes-discoverer",
    
    "relUrl": "/consumer/ldes-discoverer"
  },"134": {
    "doc": "Azure Blob Out",
    "title": "Azure Blob Out",
    "content": "LDIO Component Name: Ldio:AzureBlobOut see reference guide Apache Nifi Component Name: PutAzureBlobStorage see Apache Nifi reference guide . The Azure Blob Out writes out messages to an Azure Blob Container. Messages can be written in any format supported by Apache Jena or in JSON format. {.warning} For the json format it is necessary to have a URI which holds the context. graph LR LDES --&gt; C[Client] C --&gt; H[Azure Blob Out Component] H --&gt; S[Azure Blob Container] subgraph Publishing Pipeline C H end . ",
    "url": "/consumer/outputs/azure-blob-out",
    
    "relUrl": "/consumer/outputs/azure-blob-out"
  },"135": {
    "doc": "Console Out",
    "title": "Console Out",
    "content": "LDIO Component Name: Ldio:AzureBlobOut see reference guide Apache Nifi Component Name: LogMessage see Apache Nifi reference guide . The Console Out will output its given model to the console. In Apache Nifi, LogMessage processor, will emit a log message at the specified log level. ",
    "url": "/consumer/outputs/console-out",
    
    "relUrl": "/consumer/outputs/console-out"
  },"136": {
    "doc": "HTTP Out",
    "title": "HTTP Out",
    "content": "LDIO Component Name: Ldio:HttpOut see reference guide Apache Nifi Component Name: InvokeHTTP see Apache Nifi reference guide . The LDIO HTTP Out is a basic Http Client that will send the given Linked Data model to a target url. This pipeline component is responsible for sending harvested LDES members to an external destination (url) using the HTTP (Hypertext Transfer Protocol). HTTP is the foundational protocol used for transmitting data over the internet, primarily used for loading web pages in a browser, but it’s also widely utilized in various other types of network communication. graph LR LDES --&gt; C[Client] C --&gt; H[LDIO HTTP out] H --&gt; S[url] subgraph Publishing Pipeline C H end . ",
    "url": "/consumer/outputs/http-out",
    
    "relUrl": "/consumer/outputs/http-out"
  },"137": {
    "doc": "4. Output components",
    "title": "Linked Data Interactions Outputs",
    "content": "The LDI Output is a component that will take in Linked Data and will export it to external sources. ",
    "url": "/consumer/outputs/index#linked-data-interactions-outputs",
    
    "relUrl": "/consumer/outputs/index#linked-data-interactions-outputs"
  },"138": {
    "doc": "4. Output components",
    "title": "4. Output components",
    "content": " ",
    "url": "/consumer/outputs/index",
    
    "relUrl": "/consumer/outputs/index"
  },"139": {
    "doc": "Kafka Out",
    "title": "Kafka Out",
    "content": "LDIO Component Name: Ldio:LdioKafkaIn see reference guide Apache Nifi Component Name: ConsumeKafka see Apache Nifi reference guide . The Kafka Out sends messages to a kafka topic. graph LR LDES --&gt; C[Client] C --&gt; H[LDIO Kafka Out Component] H --&gt; S[Kafka topic] subgraph Publishing Pipeline C H end . ",
    "url": "/consumer/outputs/kafka-out",
    
    "relUrl": "/consumer/outputs/kafka-out"
  },"140": {
    "doc": "Repository Materialization",
    "title": "Repository Materialiser",
    "content": "LDIO Component Name: Ldio:RepositoryMaterialiser see reference guide Apache Nifi Component Name: RepositoryMaterialiser see reference guide . The repository materialiser is used to materialise an LDES stream into a triplestore. Any triplestore that supports the RDF4J remote repository API can be used. graph LR L[LDES members] --&gt; H[Repository Materialiser] H --&gt; S[Tripple Store] subgraph Publishing Pipeline H end . Batching . To increase the performance of this materialiser, members will be committed in batch to the triple store. However, it’s important to notice that this can have an impact on the data integrity. First of all, there could be a delay, with a maximum delay of the configured batch timeout, when the triple store will be up-to-date. Secondly, if something goes wrong halfway of a batch, all the members in that batch will not be committed to triple story and thus will be gone. So the more important data integrity is, the lower the batch-size and batch-timeout should be configured. If a more performant repository materialiser is desired, batch-size and batch-timeout should be configured somewhat higher. ",
    "url": "/consumer/outputs/repository-materialiser#repository-materialiser",
    
    "relUrl": "/consumer/outputs/repository-materialiser#repository-materialiser"
  },"141": {
    "doc": "Repository Materialization",
    "title": "Repository Materialization",
    "content": " ",
    "url": "/consumer/outputs/repository-materialiser",
    
    "relUrl": "/consumer/outputs/repository-materialiser"
  },"142": {
    "doc": "3. Transformer components",
    "title": "Linked Data Interactions Transformers",
    "content": "The LDI Transformer is a component that takes in a Linked Data model, transforms/modifies it and then puts it back on the pipeline. ",
    "url": "/consumer/transformers/index#linked-data-interactions-transformers",
    
    "relUrl": "/consumer/transformers/index#linked-data-interactions-transformers"
  },"143": {
    "doc": "3. Transformer components",
    "title": "3. Transformer components",
    "content": " ",
    "url": "/consumer/transformers/index",
    
    "relUrl": "/consumer/transformers/index"
  },"144": {
    "doc": "SPARQL Construct",
    "title": "SPARQL Construct",
    "content": "LDIO Component Name: Ldio:SparqlConstructTransformer see reference guide Apache Nifi Component Name: SPARQL Interactions Processors see reference guide . The SPARQL Construct Transformer will modify the model based on the given SPARQL Construct Query. SPARQL Construct is a query language used in semantic Web technologies to create RDF (Resource Description Framework) graphs from existing RDF data. It allows users to specify a pattern of data they wish to extract from the RDF data and construct a new graph based on that pattern. The SPARQL Construct query language provides a powerful way to create new RDF data by using existing data as the input. It can be used to transform RDF data into different formats, as well as to simplify the structure of RDF data by aggregating or filtering data. This SPARQL Construct Transfomer building block can be used to execute model transformations. ",
    "url": "/consumer/transformers/sparql-construct",
    
    "relUrl": "/consumer/transformers/sparql-construct"
  },"145": {
    "doc": "SPARQL Construct",
    "title": "Splitting models using SPARQL Construct",
    "content": "This component can be used to split models into multiple models using graphs. For example, the below query will create a dataset containing multiple models defined by ‘GRAPH’. The SPARQL construct component will extract all named models from the dataset and add all statements from the default model. The component will then return a collection of models. CONSTRUCT { GRAPH ?s { ?s ?p ?o } } WHERE { ?s ?p ?o } . ",
    "url": "/consumer/transformers/sparql-construct#splitting-models-using-sparql-construct",
    
    "relUrl": "/consumer/transformers/sparql-construct#splitting-models-using-sparql-construct"
  },"146": {
    "doc": "SPARQL Construct",
    "title": "SPARQL functions",
    "content": "We support some additional geo functions that can call inside your SPARQL Construct query, . with the following namespace: . prefix geoc: https://opengis.net/def/function/geosparql/custom# . | Function | Description | Input | Output | . | geoc:lineAtIndex | get LineString from MultiLineString by index. | MultiLineString(wktLiteral) &amp; index | LineString(wktLiteral) | . | geoc:firstCoordinate | get first Coordinate of LineString. | LineString(wktLiteral) | Coordinate(wktLiteral) | . | geoc:lastCoordinate | get last Coordinate of LineString. | LineString(wktLiteral) | Coordinate(wktLiteral) | . | geoc:lineLength | calculate total line length of LineString. | LineString(wktLiteral) | distance in meters | . | geoc:midPoint | calculate midpoint of LineString. | LineString(wktLiteral) | Coordinate(wktLiteral) | . | geoc:pointAtFromStart | calculate point on LineString by distance. | LineString(wktLiteral) &amp; distance in meters | Coordinate(wktLiteral) | . |   |   |   |   | . ",
    "url": "/consumer/transformers/sparql-construct#sparql-functions",
    
    "relUrl": "/consumer/transformers/sparql-construct#sparql-functions"
  },"147": {
    "doc": "Intermediary Pipeline",
    "title": "Intermediary Pipeline",
    "content": ". A data intermediary, particularly one specialized in working with Linked Data Event Streams (LDES), plays a pivotal role in the data space ecosystem. These intermediaries act as the connective tissue between data publishers and consumers, focusing on the acquisition, enhancement, and redistribution of data streams. Their work with LDES involves not just the simple relaying of data from one point to another but adding substantial value to the data. This can include cleaning and restructuring data, enriching it with additional context or information from other sources, and ensuring the data is modeled and published in a way that adheres to Linked Data principles. This ensures that the data is not only more useful and insightful for end-users but also interoperable within the broader web of data. The ultimate goal of a data intermediary working with LDES is to facilitate a seamless, efficient, and enriching flow of data across different sectors and users. By doing so, they help unlock the full potential of Linked Data for businesses, researchers, and public entities, making data more accessible, understandable, and actionable. As a data intermediary, your objective is to collect LDES streams and then release a revamped, enriched LDES data stream. The capability to utilize data processing techniques for cleaning, restructuring, or augmenting the initial datasets requires the establishment of a specialized LDES consumption pipeline. Your role involves implementing a resilient and scalable pipeline that not only facilitates an uninterrupted data flow but also aligns with Linked Data principles, thereby transforming your enhanced LDES stream into a crucial resource within the data ecosystem. ",
    "url": "/intermediary/index",
    
    "relUrl": "/intermediary/index"
  }
}
